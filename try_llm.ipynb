{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from model_client import ModelClient\n",
    "from pdf_extraction import extract_text_from_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are several libraries and tools available for extracting text from PDF files in Python. Here are some of the most popular ones:\\n\\n1. **PyPDF2**: A library that provides an easy-to-use interface for reading and writing PDF files, including extracting text.\\n\\n2. **pdfminer**: A comprehensive tool for extracting text and layout information from PDF files.\\n\\n3. **Tesseract-OCR**: An Optical Character Recognition (OCR) engine developed by Google that can extract text from scanned or image-based PDFs.\\n\\n4. **Pytesseract**: A Python wrapper for Tesseract-OCR that makes it easy to integrate OCR functionality into your scripts.\\n\\n5. **pdfquery**: A library that allows you to search and extract data from PDF files using XPath expressions.\\n\\n6. **pdfrw**: A library that provides a simple way to read, write, and manipulate PDF files.\\n\\n7. **pdfplumber**: A modern library for extracting text and layout information from PDFs with a focus on ease of use and high accuracy.\\n\\n8. **camelot**: A Python package to extract tables from pdf.\\n\\nHere is an example using Pytesseract and pdfminer:\\n\\n```python\\nimport pytesseract\\nfrom PIL import Image\\nfrom pdfminer.converter import TextConverter\\nfrom pdfminer.layout import LAParams\\nfrom pdfminer.pdfdocument import PDFDocument\\nfrom pdfminer.pdfpage import PDFPage\\nfrom pdfminer.pdfparser import PDFParser\\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageAggregator\\n\\n# Open the PDF file\\nwith open('file.pdf', 'rb') as f:\\n    parser = PDFParser(f)\\n    doc = PDFDocument(parser)\\n\\n    # Get the PDF document information\\n    info = doc.get_info()\\n\\n    # Create a PDF resource manager\\n    rsrcmgr = PDFResourceManager()\\n\\n    # Create a text converter\\n    device = TextConverter(rsrcmgr, LAParams(), document=doc)\\n\\n    # Process the PDF pages one by one\\n    for page in PDFPage.create_pages(doc):\\n        if page:\\n            # Extract the text from each page\\n            pageaggregator = PDFPageAggregator(device)\\n            layout = pageaggregator.get_result()\\n            text = ''\\n            for elem in layout:\\n                if isinstance(elem, pdfminer.layout.LTTextContainer):\\n                    text += elem.get_text()\\n            yield text\\n\\n# Use Tesseract-OCR to extract text from the pages\\nfor text in doc.get_pages():\\n    print(pytesseract.image_to_string(Image.open(text)))\\n```\\n\\nAnd here is an example using pdfplumber:\\n\\n```python\\nimport pdfplumber\\n\\nwith pdfplumber.open('file.pdf') as pdf:\\n    for page in pdf.pages:\\n        text = page.extract_text()\\n        print(text)\\n```\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = ModelClient()\n",
    "prompt = \"What are the best tools for extracting text from pdf in Python?\"\n",
    "client.ask_llama(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Citation: Al-Hussaini, I.; Mitchell,\\nC.S.SeizFt : Interpretable Machine\\nLearning for Seizure Detection Using\\nWearables. Bioengineering 2023 ,10,\\n918. https://doi.org/10.3390/\\nbioengineering10080918\\nAcademic Editors: Chen Chen, Wei\\nChen, Maarten De Vos and Christos\\nChatzichristos\\nReceived: 20 June 2023\\nRevised: 28 July 2023\\nAccepted: 31 July 2023\\nPublished: 2 August 2023\\nCopyright: © 2023 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\nbioengineering \\nArticle\\nSeizFt : Interpretable Machine Learning for Seizure Detection\\nUsing Wearables\\nIrfan Al-Hussaini1\\nand Cassie S. Mitchell2,3,*\\n1School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA\\n2Department of Biomedical Engineering, Georgia Institute of Technology and Emory University,\\nAtlanta, GA 30332, USA\\n3Machine Learning Center at Georgia Tech, Georgia Institute of Technology, Atlanta, GA 30332, USA\\n*Correspondence: cassie.mitchell@bme.gatech.edu\\nAbstract: This work presents SeizFt —a novel seizure detection framework that utilizes machine\\nlearning to automatically detect seizures using wearable SensorDot EEG data. Inspired by inter-\\npretable sleep staging, our novel approach employs a unique combination of data augmentation,\\nmeaningful feature extraction, and an ensemble of decision trees to improve resilience to variations\\nin EEG and to increase the capacity to generalize to unseen data. Fourier Transform (FT) Surrogates\\nwere utilized to increase sample size and improve the class balance between labeled non-seizure and\\nseizure epochs. To enhance model stability and accuracy, SeizFt utilizes an ensemble of decision\\ntrees through the CatBoost classiﬁer to classify each second of EEG recording as seizure or non-\\nseizure. The SeizIt1 dataset was used for training, and the SeizIt2 dataset for validation and testing.\\nModel performance for seizure detection was evaluated using two primary metrics: sensitivity using\\nthe any-overlap method (OVLP) and False Alarm (FA) rate using epoch-based scoring (EPOCH).\\nNotably, SeizFt placed ﬁrst among an array of state-of-the-art seizure detection algorithms as part of\\nthe Seizure Detection Grand Challenge at the 2023 International Conference on Acoustics, Speech,\\nand Signal Processing (ICASSP). SeizFt outperformed state-of-the-art black-box models in accurate\\nseizure detection and minimized false alarms, obtaining a total score of 40.15, combining OVLP and\\nEPOCH across two tasks and representing an improvement of \\x1830% from the next best approach. The\\ninterpretability of SeizFt is a key advantage, as it fosters trust and accountability among healthcare\\nprofessionals. The most predictive seizure detection features extracted from SeizFt were: delta\\nwave, interquartile range, standard deviation, total absolute power, theta wave, the ratio of delta to\\ntheta, binned entropy, Hjorth complexity, delta + theta, and Higuchi fractal dimension. In conclusion,\\nthe successful application of SeizFt to wearable SensorDot data suggests its potential for real-time,\\ncontinuous monitoring to improve personalized medicine for epilepsy.\\nKeywords: seizure; EEG; augmentation; xai; interpretability; imbalanced classes; electroencephalogram ;\\nartiﬁcial intelligence; machine learning\\n1. Introduction\\nSeizures are abnormal, uncontrolled electrical discharges in the brain that can mani-\\nfest as various symptoms ranging from mild to severe [ 1,2]. Common seizure symptoms\\ninclude involuntary muscle movements, partial to full loss of consciousness, and cognitive\\ndisturbances, especially in the ictal and post-ictal phases [ 3,4]. Seizure events are partic-\\nularly relevant for individuals suffering from epilepsy, a chronic neurological disorder\\ncharacterized by recurrent seizures. The timely and accurate detection of seizures is crit-\\nical for the appropriate diagnosis and management of epilepsy. Seizure detection using\\nwearables enables healthcare professionals to devise tailored treatment plans and monitor\\nthe efﬁcacy of therapeutic interventions [ 5–8]. Furthermore, the identiﬁcation of seizure\\npatterns can help prevent potential injury and improve the overall quality of life of epilepsy\\npatients [9–11].\\nBioengineering 2023 ,10, 918. https://doi.org/10.3390/bioengineering10080918 https://www.mdpi.com/journal/bioengineering\\nBioengineering 2023 ,10, 918 2 of 16\\nIn clinical practice, the gold standard for seizure detection is the Electroencephalogram\\n(EEG). The EEG is a non-invasive technique that records electrical activity in the brain via\\nelectrodes placed on the scalp in a speciﬁc pattern that correlates with the lobes of the brain.\\nTrained clinical neurophysiologists analyze the EEG recordings to identify speciﬁc patterns\\nindicative of seizures and to attempt to localize the originating location of the seizure\\nactivity [ 12–14]. However, this manual approach is time-consuming, requires extensive\\nexpertise, and can be prone to human error, particularly when analyzing very large volumes\\nof data. Moreover, the ﬁnancial burden associated with conducting and interpreting EEGs\\nfurther highlights the need for more efﬁcient and cost-effective methods.\\nMachine learning (ML) and artiﬁcial intelligence (AI) have the potential to revolution-\\nize the ﬁeld of seizure detection by automating the analysis of EEG recordings, thereby\\ndrastically reducing the associated time and cost [ 15,16]. By leveraging advanced algo-\\nrithms capable of identifying complex patterns in large datasets [ 17–21], ML-based systems\\ncan achieve high levels of accuracy. In fact, in some instances, ML surpasses human perfor-\\nmance. Recently developed wearable EEG devices are more portable and patient-friendly.\\nFor example, behind-the-ear electrodes can enable continuous, real-time monitoring of\\nbrain activity for prompt intervention and optimal epilepsy management [22–25].\\nIn recent years, deep learning methodologies have demonstrated signiﬁcant potential\\nfor effective seizure detection [ 26–29]. Deep learning is a subset of machine learning that\\nemploys artiﬁcial neural networks with many hidden layers. Such artiﬁcial neural networks\\nlearn hierarchical representations of input data, which makes them particularly suitable for\\nanalyzing complex, high-dimensional EEG signals [ 30–34]. Some deep learning models\\nhave obtained remarkable accuracy for detecting seizures, such as Convolutional Neural\\nNetwork (CNN) [ 28], Recurrent-CNN (RCNN) [ 26], and auto-encoders [ 27]. However,\\ndeep learning models often lack interpretability, which is crucial for fostering trust and\\naccountability in clinical settings [35–39].\\nThe presented SeizFt work expands upon research conducted as part of the Seizure\\nDetection Challenge 2023 (https://signalprocessingsociety.org/publications-resources/dat\\na-challenges/seizure-detection-challenge-icassp-2023 (accessed on 30 July 2023)), a Grand\\nChallenge at the International Conference on Acoustics, Speech, and Signal Processing\\n(ICASSP). The Grand Challenge, which is detailed in Section 2, aimed to develop advanced\\nmachine learning frameworks capable of accurately detecting seizures in patients with\\nepilepsy using EEG data collected from a discreet wearable device equipped with behind-\\nthe-ear electrodes. The Grand Challenge was divided into two distinct tasks, each targeting\\na different aspect of seizure detection: Task 1 focused on the development of a machine\\nlearning model for detecting seizures in wearable SensorDot (SD) data, while Task 2 centered\\naround the optimization of a given Deep Learning model for wearable seizure detection.\\nThe SeizFt framework aims to revolutionize epilepsy management by offering a\\nmore efﬁcient, accurate, and interpretable approach to seizure detection. By combining\\nrobust features, data augmentation, class balancing, and an interpretable machine learning\\nalgorithm, we demonstrate the feasibility of developing advanced seizure detection sys-\\ntems that can signiﬁcantly improve the quality of life for individuals living with epilepsy.\\nThe SeizFt approach addresses the need for transparency and understandability while\\nmaintaining high levels of accuracy.\\nTheSeizFt framework leverages a variety of robust features extracted from the EEG\\ndata that were inspired by recent work in interpretable sleep staging [ 39,40]. These features\\ninclude Standard Deviation (STD), Inter-Quartile Range (IQR), Skewness, Kurtosis, Number\\nof Zero Crossings, Hjorth mobility and complexity, Fractal dimensions, Entropies, and the\\nPower in Different Energy Bands, such as Delta. Such features provide comprehensive\\ninformation about the underlying characteristics of the EEG signals that enable the model\\nto make more accurate and interpretable predictions.\\nData augmentation and class balancing play a crucial role in improving the per-\\nformance of our SeizFt framework. In this study, we employ Fourier Transform (FT)\\nSurrogates [ 41,42] to augment the EEG signals during training and balance the number\\nBioengineering 2023 ,10, 918 3 of 16\\nof seizure and non-seizure epochs. This technique signiﬁcantly helps in addressing the\\nchallenges associated with imbalanced datasets, which are commonly found in medical\\napplications. For example, in continuous epilepsy monitoring, seizure events are very\\nrare compared to non-seizure epochs. Moreover, the augmentation strategy enhances\\nthe generalization capabilities of the model. As such, the model still performs well on\\nunseen data.\\nTheSeizFt framework is built upon an ensemble of trees using CatBoost [ 43,44]. The\\nensemble approach combines the predictions of multiple trees, leading to a more accurate\\nand robust model. Additionally, CatBoost effectively handles class imbalance by assigning\\nweights to the classes during training, which further improves the model’s performance in\\ndetecting seizures.\\nIn our experiments, we demonstrate the effectiveness of the SeizFt framework.\\nSeizFt is compared with other state-of-the-art approaches, such as ChronoNet [ 45] and a\\nproposed deep neural network with multi-headed attention, referred to as AttentionNet [46] .\\nResults illustrate that SeizFt achieves superior sensitivity and lower false alarm rates on\\nthe SeizeIT2 dataset [ 47], resulting in an overall improvement of 30% from state-of-the-art\\nbenchmarks. Utilizing our proposed SeizFt framework for Task 1 and our presented\\naugmentation and class balancing strategy for Task 2, we obtained scores of 47.57 and\\n29.01, respectively, as evaluated on the holdout test, SeizeIT2 [ 47] provided by the challenge\\nadministrators. These combined efforts resulted in a total score of 40.15, marking our sub-\\nmission as the only submitted method that exceeded the established benchmark of 31.03,\\nwhich was set by the Grand Challenge organizers. Finally, we provide an interpretation of\\nthe most important features used by the SeizFt model, which offers valuable insights into\\nthe characteristics of seizures as captured by EEG signals.\\nIn summary, this paper makes several key contributions to the ﬁeld, as outlined below:\\n• First, we propose a deep learning model that combines CNN, Long Short-Term Mem-\\nory (LSTM), and multi-headed attention elements outperforming traditional deep\\nlearning methods.\\n• We propose SeizFt , a robust and explainable seizure detection method for wearable\\nEEG devices. This framework merges several computational strategies, including\\nfeature extraction, data augmentation via Fourier Transform (FT) Surrogates, class\\nbalancing, and a CatBoost-driven ensemble of decision trees.\\n• By means of comprehensive experimental analysis, we validate the superior perfor-\\nmance of SeizFt . We show that our model excels in terms of sensitivity and false\\nalarm rates, consistently outperforming other established state-of-the-art methods.\\n• Importantly, we highlight the vital, clinically interpretable features that SeizFt em-\\nploys to characterize seizures as captured by EEG. These critical features underline\\nthe interpretability of our model, enhancing trust in its predictive ability and marking\\na signiﬁcant advance in the integration of machine learning within a clinical context.\\n• Finally, we consider the practical implications and future potential of SeizFt , asserting\\nthat it establishes a new benchmark for seizure detection using wearable EEG and\\nsuggest possible directions for future research and application. This, we believe, will\\ninspire advancements that could profoundly impact patient care.\\n2. The 2023 ICASSP Seizure Detection Grand Challenge\\nThis presented work for SeizFt represents the winning submission to the Seizure\\nDetection Challenge 2023, a Grand Challenge hosted by the International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP). This section details the Grand Challenge\\ntasks and datasets, which inspired the development of SeizFt .\\n2.1. Tasks\\nThere were two core tasks: Seizure Detection and Data-Centric Seizure Detection.\\nAdditionally, two speciﬁc datasets were provided, as described below.\\nBioengineering 2023 ,10, 918 4 of 16\\nThe Seizure Detection task (Task 1) required the creation of a machine learning (ML)\\nmodel to identify seizures using wearable SD data. For training this model, we used the\\nSeizeIT1 dataset [ 48]. However, the test set comprised data gathered through the wearable\\nSD device. We had the option, but not the obligation, to leverage the full scalp EEG (vEEG)\\ndata available in the training set. The pre-trained model was built to encompass routines\\nemployed for pre and post-processing. We fed the model with wearable EEG and/or\\nsingle-channel ECG data from the SD device. The ﬁnal aim was to assign a seizure or\\nnon-seizure label for each second in the recording.\\nThe second task, Data-Centric Seizure Detection (Task 2), concentrated on the ’data-\\ncentric AI’ approach. This task emphasized the signiﬁcance of data quality and the rep-\\nresentation of a variety of cases within the training set for optimizing seizure detection\\nperformance. Unlike Task 1, the goal here was to apply data manipulation techniques\\nto enhance the performance of the model provided for wearable seizure detection. For\\nthis task, we utilized the same training set as in Task 1 and were given a Deep Learning\\nmodel—an adapted version of the ChronoNet [ 45]. This model was designed to accept a\\ntwo-second, two-channel EEG window resampled at a sampling frequency of 200 Hz. The\\nmodel was expected to output a binary vector where each consecutive two-second EEG\\nsegment was marked as a seizure or non-seizure period.\\n2.2. Data Sources\\nThe data used in this study is sourced from two EEG datasets— SeizeIt1 [48] and\\nSeizeIT2 [47]. Summaries of the data utilized in this study are provided below. For addi-\\ntional details, please refer to the datasets’ originally published works [47,48].\\n2.2.1. SeizeIt1 Dataset—Training Set\\nThe SeizeIt1 [ 48] dataset was collected during an ICON project (2017–2018) through\\ncollaboration between KU Leuven (ESAT-STADIUS), UZ Leuven, UCB, Byteﬂies, and\\nPilipili. The goal of the project was to design a home environment patient monitoring\\nsystem using behind-the-ear (bhe) EEG electrodes. The data were collected in the hospital\\nduring presurgical evaluation. The dataset includes a full 10–20 scalp EEG data, behind-\\nthe-ear data, and single-lead ECG data. Seizure annotations were performed by clinicians\\nbased on the gold standard vEEG system.\\nThe data of 82 patients were recorded between 23 January 2017 and 26 October 2018.\\nOf these patients, 54 were recorded with the behind-the-ear channels. In total, 42 of those\\npatients experienced seizures during their presurgical evaluation. The number of seizures\\nper patient ranged from 1 to 22, with a median of 3 seizures per patient. The duration of\\nthe seizures, the time difference of seizure EEG onset and end, varied between 11 and 695 s\\nwith a median of 50 s. In all, 89% of the seizures were Focal Impaired Awareness seizures,\\nand 91% of the seizures originated from the (fronto-) temporal lobe.\\nThe dataset made available to 2023 ICASSP Seizure Detection Grand Challenge partic-\\nipants contained data from the 42 patients who experienced epileptic events during the\\nrecording period.\\n2.2.2. SeizeIT2 Dataset—Validation and Test Set\\nThe SeizeIT2 dataset, an extension of the SeizeIT1 project, contains EEG and ECG record-\\nings of more than 350 patients with epilepsy, making use of a wearable Seizure Detection\\n(SD) device for in-home and hospital seizure monitoring. It represents the first phase-4\\nclinical trial of wearable in-home monitoring for seizure detection. It brings together public\\nand private stakeholders, including academic institutions such as Université de Navarra,\\nKarolinska Institutet, KU Leuven, RWTH Aachen, Universitäts klinikum Aachen, Centro\\nHospitalar e Universitário de Coimbra, Oxford University Hospitals NHS Foundation Trust,\\nand Stockholms Läns Landsting, led by UCB. The focus of this dataset is the clinical validation\\nof the SD in patients experiencing typical absence, focal impaired awareness, and generalized\\ntonic-clonic seizures. Patients underwent video-EEG monitoring with additional electrodes\\nBioengineering 2023 ,10, 918 5 of 16\\nattached behind each ear for concomitant SD recording. The SD device was secured on the\\nupper back using a patch, and the impedance was maintained at \\x145 kW.\\nIn the context of the 2023 ICASSP Seizure Detection Grand Challenge, the SensorDot\\ndata of the patients in UZ Leuven was used for the test set, as well as for the validation set.\\nThe SensorDot recordings from 2 patients were provided to the participants to validate the\\nperformance of their models (Table 1). All the models were tested by the Grand Challenge\\norganizers in the remaining 31 adult patients from UZ Leuven who had focal seizures\\n(Table 2).\\n2.3. Performance Metrics\\nIn seizure detection, sensitivity and false alarm (FA) rates are typically prioritized as\\nperformance metrics due to the seizure event’s rarity. These metrics offer a more precise\\nmeasure of the model’s ability to correctly identify true seizure events, which is crucial.\\nForSeizFt , model performance is evaluated using two primary metrics: Sensitivity using\\nthe any-overlap method (OVLP) [ 49] and False Alarm (FA) rate using epoch-based scoring\\n(EPOCH) [ 49]. Sensitivity measures the proportion of correctly identiﬁed seizure epochs,\\nwhile the FA rate quantiﬁes the number of false alarms per hour. To balance sensitivity\\nand FA rate, a weighting factor of \\x000.4 is applied. The overall performance score is then\\ncalculated as the weighted average of Task 1 and Task 2 scores, with weights of 0.6 for\\nTask 1 and 0.4 for Task 2. The scoring formula is as follows:\\nScore Task x =SensitivityTask x\\x000.4 FA Task x /hour (1)\\nTotal_Score =0.6 Score Task 1 +0.4 Score Task 2 (2)\\nEquation (1) serves to compute the separate scores for both Task 1 and Task 2. These\\nindependent results are subsequently merged using Equation (2) to deduce the comprehen-\\nsive total score. As such, it is worth noting that Task 1’s score holds a heavier weight in the\\ndetermination of the overall score.\\n3. Methods\\nHere we present the framework, construction, and other methods utilized to develop\\nSeizFt . The speciﬁc objective of SeizFt was to surpass deep learning model accuracy\\nwith an algorithm that enables transparent analysis of clinically interpretable features. The\\nwork was split into two tasks that aligned with the previously stated 2023 ICASSP Seizure\\nDetection Grand Challenge tasks: seizure detection and data-centric seizure detection.\\n3.1.SeizFt Model Framework\\nWe propose SeizFt , a framework for interpreting features in seizure detection as part\\nof Task 1. This model’s success hinges on a systematic sequence of steps used during the\\ntraining process to bolster both the model’s accuracy and its interpretability. The framework\\nis illustrated in Figure 1. Our approach is outlined in several stages:\\nFigure 1. SeizFt Framework.\\n3.1.1. Data Augmentation and Class Balancing\\nA common issue in biomedical signal analysis, including EEG signals used in seizure\\ndetection, is the quality and quantity of available data. Often, the amount of data is\\ninsufﬁcient for training robust machine learning models. Moreover, the data may also\\nbe unbalanced. Unbalanced data has far more observations for one label (or class) type\\nBioengineering 2023 ,10, 918 6 of 16\\ncompared to another. For example, in epileptic EEG monitoring, there is typically much\\nmore data collected during non-seizure periods than during periods of seizure. To mitigate\\nissues with sample size and class imbalance, we employ data augmentation techniques.\\nData augmentation is a relatively common machine learning technique used to improve\\nmodel performance and, in particular, to improve the model’s ability to generalize to new,\\nunseen data [50–52].\\nIn the context of our work, Fourier Transform (FT) Surrogates are used for data\\naugmentation [ 41,42]. FT Surrogates is a mathematical technique that transforms a function\\nof time, such as an EEG signal, into a function of frequency. This transformed function, or\\nspectrum, provides a different perspective on the data and highlights different aspects of\\nthe underlying signal.\\nFT Surrogates are a speciﬁc type of data augmentation where surrogate data are\\ngenerated by randomizing the phases of the original EEG signal’s Fourier Transform.\\nCrucially, this is done while preserving the power spectrum, which is a measure of the\\nsignal’s power intensity in the frequency domain. This ensures that the overall structure of\\nthe original signal is maintained, even as the temporal organization of the signal (e.g., the\\nspeciﬁc order and timing of events) is altered.\\nThe use of FT Surrogates serves two primary purposes. Firstly, it increases the size of\\nthe training dataset. By creating additional, varied examples from the existing data, the\\nmodel has more material from which to learn, which improves its performance. Secondly,\\nthis method adds diversity to the training dataset. The surrogate data are similar to but\\ndistinct from, the original data. As such, they allow the model to learn to recognize seizures\\nin a broader range of circumstances.\\nMoreover, FT Surrogates help to address class imbalance, a common issue in biomedi-\\ncal machine learning. In many datasets, there are more examples of non-seizure periods\\nthan seizure periods, which can skew the model’s learning. By generating additional\\nseizure and non-seizure epochs using FT Surrogates, we can balance the distribution of\\nclasses in the training data. As a result, the model learns to recognize both seizure and\\nnon-seizure periods equally well.\\n3.1.2. Feature Extraction\\nThis study’s approach to EEG feature extraction is inspired by recent advancements\\nin interpretable sleep staging [ 39,40]. These advancements have yielded methods for ef-\\nfectively and robustly extracting features from EEG signals that capture the underlying\\ncharacteristics of the signals. The speciﬁc features extracted in the present study are catego-\\nrized into four main types: statistical measures, temporal features, complexity measures,\\nand spectral features.\\nStatistical measures used were Standard Deviation (STD), Interquartile Range (IQR),\\nSkewness, and Kurtosis. STD and IQR provide a measure of variability in the EEG signals.\\nSTD quantiﬁes the average distance of the data, and IQR provides a measure of statistical\\ndispersion. Skewness and kurtosis describe the shape of the signal’s probability distribution.\\nSkewness measures the asymmetry, and kurtosis assesses the heaviness of the signal’s tails\\nrelative to a normal distribution.\\nTemporal features considered include the Number of Zero Crossings, Hjorth mobility,\\nand complexity. The number of zero crossings provides an indication of the signal’s\\nfrequency, while Hjorth parameters (mobility and complexity) are descriptive statistics\\nused to characterize the signal. Speciﬁcally, mobility measures the mean frequency or the\\nsignal’s rate of change. Complexity compares the signal’s similarity to a pure sine wave.\\nThe complexity measures utilized were Fractal dimensions and Entropies. Fractal\\ndimensions measure how the detail in the data changes with the scale. On the other hand,\\nentropy provides a quantitative measure of the signal’s randomness or unpredictability.\\nSpectral features, such as power in different energy bands, such as Delta, Theta, Alpha,\\nand Beta, are also extracted. These spectral features divide the total EEG signal into different\\nfrequency bands and measure the relative power within each band. These energy bands\\nBioengineering 2023 ,10, 918 7 of 16\\nrepresent different brain states and activities. For instance, Delta waves are associated with\\ndeep, dreamless sleep and regeneration; Theta with creativity and insight; Alpha with\\nrelaxed awareness; and Beta with active thinking and focus [53].\\nThe combination of these features presents a comprehensive representation of the\\nEEG signals that enables the model to effectively discern between seizure and non-seizure\\nepochs. We maximize the potential to detect and classify seizure activity within the data by\\ncapturing the nuances of the signals through this diverse set of features. The comprehensive\\nand robust feature extraction method forms a strong foundation for the development of an\\neffective seizure detection model.\\n3.1.3. Model Training\\nFor the Seizure Detection Task , orTask 1 , we used CatBoost [ 43,44]—a machine learning\\nalgorithm based on gradient boosting over decision trees—to classify seizures and non-\\nseizures in each epoch. It is an ensemble learning method that performs exceptionally well\\nacross a broad spectrum of tasks by iteratively combining hundreds or thousands of weak\\nlearners to generate a powerful single predictive model.\\nOne of the crucial challenges in seizure detection using machine learning is the severe\\nclass imbalance typically present in the data. Seizures are rare events and hence, are vastly\\noutnumbered by non-seizure epochs. To address this issue, we integrated weights into\\nthe CatBoost model. These weights were derived from the distribution of seizure and\\nnon-seizure epochs after the application of the augmentation strategy, FT Surrogate [41].\\nThe Data-Centric Seizure Detection Task , orTask 2 , also faced the problem of class imbal-\\nance between seizure and non-seizure epochs. Again, the FT Surrogate methodology was\\nutilized for EEG data augmentation. Moreover, the relative class weights were dynamically\\nupdated prior to the model’s training phase. This approach was based on the recognition\\nthat the quality of the training data and their ability to represent diverse cases is crucial for\\nthe success of machine learning applications. By augmenting the data and recalibrating\\nthe class weights, SeizFt created a training set that robustly represents both seizure and\\nnon-seizure data.\\nIn essence, this two-pronged approach ensured that the deep learning model was\\neffectively optimized for wearable seizure detection. Simultaneously, it maintained a\\nbalance between seizure and non-seizure data representation, allowing for a more reliable\\nand accurate model for seizure detection.\\n3.2. Experimental Setup\\nOur study employed the SeizeIT1 dataset [ 48], which comprised EEG data from\\nepilepsy patients that was clearly annotated to indicate seizure and non-seizure epochs.\\nThis dataset was subjected to a random split, with an 8:2 ratio, which formed the training\\nand validation sets, respectively.\\nOur model’s performance was further tested on the SeizeIT2 dataset [ 47], which acted\\nas an independent test set. This allowed us to assess the model’s generalizability and\\nperformance on unseen data. It is important to note that some of the evaluation results\\ndetailed in subsequent sections were performed by the challenge organizers, given that we\\ndid not have access to a speciﬁc held-out test set. This arrangement ensured an unbiased\\nevaluation of our model’s performance.\\nSeveral software tools and libraries were employed throughout this study to facilitate\\nvarious stages of model development. We utilized Braindecode and MNE [ 42,54] for FT\\nSurrogate augmentation, which is a vital step in augmenting our dataset. Feature extrac-\\ntion, a critical pre-processing step in machine learning, was performed using tsﬂex [ 55],\\nscipy [56] , and scikit-learn [ 57]. These libraries provide a comprehensive set of methods for\\nextracting meaningful features from our time-series data. For the training phase, different\\nlibraries were used depending on the model. CatBoost [ 43,44] was chosen to train SeizFt .\\nFor training AttentionNet , we opted for PyTorch 2.0 [ 58]. Lastly, TensorFlow [ 59] was the\\nlibrary of choice for training ChronoNet [45].\\nBioengineering 2023 ,10, 918 8 of 16\\n3.3. Baselines\\nThe following were used as baselines to compare the performance of SeizFt to other\\nhigh-performing or state-of-the-art seizure detection models:\\n• AttentionNet : The architecture, as depicted in Figure 2, integrates a neural network\\ncomprised of CNN layers followed by LSTM layers. It is augmented with a multi-\\nheaded attention mechanism.\\n• ChronoNet [ 45]: This approach utilized a novel Recurrent Neural Network (RNN)\\narchitecture, dubbed ChronoNet, which was inspired by recent advancements in image\\nclassiﬁcation. The authors adapted the methodology to EEG data interpretation to\\nimprove the efﬁciency and accuracy in distinguishing between normal and abnormal\\nbrain activity. ChronoNet incorporates multiple 1D convolution layers followed by\\ndeep Gated Recurrent Unit (GRU) layers, taking raw time-series EEG data and learning\\nto identify patterns in brain activity.\\n• Transformer and CNN [60]: Recognizing the limitation of conventional EEG systems\\nand the need for timely and accurate diagnosis, the authors adopted a transformer-\\nbased deep neural network approach. This model incorporated a mixed Transformer\\nand CNN architecture, which was trained and pre-trained on several datasets, in-\\ncluding the Temple University Hospital Seizure Corpus [ 61]. To improve the model’s\\nrobustness and mitigate overﬁtting, dropout, and early stopping techniques were\\nemployed. For the second task, the authors enhanced the ChronoNet architecture\\nfor abnormal EEG detection by boosting the signal-to-noise ratio in EEG data and\\noptimizing various training parameters.\\n• Spectral Power and Random Forest [ 62]: This approach to epilepsy monitoring used\\nlightweight machine-learning models speciﬁcally tailored for resource-constrained\\nwearable devices. The framework uses the Random Forest (RF) algorithm in conjunc-\\ntion with power features extracted from speciﬁc EEG frequency bands for seizure\\ndetection. The power features were derived from different frequency ranges and are\\ncalculated through a combination of time-domain band-pass ﬁltering and the applica-\\ntion of Parseval’s theorem to the ﬁltered signal. For the RF algorithm, an ensemble\\nof relatively shallow decision trees is employed. The ﬁnal prediction was obtained\\nby voting on the outcomes of all the trees, which reduces computational complex-\\nity for real-time inference on wearable devices. In the second task, the data-centric\\nChronoNet architecture was utilized to optimize certain hyperparameters to balance\\nseizure detection sensitivity and false alarm rates.\\n• Deep Convolutional Neural Network [ 63]: EEG data were preprocessed, standard-\\nized, and segmented into 2-second intervals before model training. Experiments\\nwere performed with several deep CNN architectures and a range of input types,\\nsuch as raw EEG signals, Short-time Fourier Transform (STFT) transformed signals,\\nwavelets, and mel scale spectral transformations. Over 100 different DCNN models\\nwere trained. The optimal models were chosen based on the mean of lower boundaries\\nof 95% conﬁdence intervals for cross-validation and hold-out scores. Additionally,\\ndata augmentation techniques were implemented in the second task to improve the\\nperformance of the ChronoNet architecture.\\nBioengineering 2023 ,10, 918 9 of 16\\nFigure 2. AttentionNet architecture.\\n4. Results\\nThe performance and interpretability of SeizFt was evaluated and compared to other\\nstate-of-the-art automated seizure detection methods.\\n4.1. Comparative Analysis\\nTables 1 and 2 demonstrate the efﬁcacy of the SeizFt approach compared to other\\nhigh-performing, state-of-the-art methods on the same dataset. A thorough comparative\\nanalysis is presented in Table 1, which benchmarks the performance of our proposed SeizFt\\nmodel against various state-of-the-art models. Notably, these include ChronoNet [45] and\\nthe multi-headed attention deep neural network, AttentionNet (refer to Figure 2). The table\\nalso features an optimized version of the ChronoNet model speciﬁcally designed for Task 2\\nto showcase the effectiveness of data-driven methodologies in seizure detection tasks.\\nThe SeizFt model, constructed for Task 1, outperforms all other tested models in\\nsensitivity, false alarm rate, and total score. As such, SeizFt sets a new performance\\nbenchmark. The commendable performance boost of SeizFt is primarily due to the efﬁcient\\ndata augmentation and class balancing techniques adopted. Our proposed approach,\\nwhich employs the same ChronoNet model as the organizer benchmark (shown in the ﬁrst\\nrow) but adds class re-balancing and FT Surrogates, achieves an approximately threefold\\nimprovement over their baseline model, as can be seen in the second row of the table.\\nAn optimized ChronoNet model, speciﬁcally designed for Task 2 and using a similar\\nmethodology, also depicts a substantial reduction in false alarm rate and an increase in\\ntotal score compared to the baseline ChronoNet [ 45]. Collectively, these results reafﬁrm the\\nefﬁcacy of the presented SeizFt methodology.\\nBioengineering 2023 ,10, 918 10 of 16\\nTable 1. Model Evaluation on SeizeIT2 Dataset [47]a.\\nSensitivity (OVLP [49]) False Alarms per Hour (EPOCH [49]) Total Score\\nChronoNet [45] 58.22 117.12 11.37\\nAttentionNet (Figure 2) 53.57 30.85 41.23\\nSeizFt (Task 1) 62.86 14.93 56.88\\nChronoNet (Task 2) 22.22 9.82 18.30\\naBold signiﬁes the best score for the corresponding metric.\\nTable 2. Ofﬁcial Results of the ICASSP Seizure Detection Grand Challengea.\\nScore Task 1 Score Task 2 Total Score\\nBenchmark 45.10 10.42 31.03\\nPathology Dynamics ( SeizFt ) 47.57 29.01 40.15\\nUCLA CDx [60] 26.92 29.32 27.88\\nNeural Engineering Lab [62] 36.98 4.06 23.81\\nBrainify.ai [63] 6.54 25.21 14.00\\naBold signiﬁes the best score for the corresponding metric.\\nThe performance of the models was independently veriﬁed by the 2023 ICASSP\\nSeizure Detection Grand Challenge’s competition organizers using a concealed test set.\\nThis process ensured a rigorous and unbiased evaluation of all models’ generalization\\ncapabilities to unseen data. The ﬁnal confusion matrix output for SeizFt and other model\\nentries using the independent concealed test set was not made available. Instead, the\\nscoring Equations (1) and (2) shown in Section 2 were used to compare model performance\\nand generalizability to the unseen concealed test set. Despite the limited access to the\\nevaluation dataset, the provided comparative analysis illustrates the superior performance\\nofSeizFt over other state-of-the-art methods, including the organizer Benchmark model.\\nThe ﬁnal evaluation results for the 2023 ICASSP Seizure Detection Grand Challenge\\nare presented in Table 2. Note that our entry in the 2023 ICASSP Seizure Detection Grand\\nChallenge ofﬁcial results was listed under our research laboratory name, “Pathology Dy-\\nnamics”. To be clear, the listed Pathology Dynamics entry is SeizFt . The SeizFt approach\\nfor Task 1 and Task 2 achieves the highest [or best] total score of 40.15. Thus, SeizFt\\nwas the only method to surpass the 2023 ICASSP Seizure Detection Grand Challenge\\norganizers’ benchmark, which scored 31.03. This achievement underscores the effective-\\nness of the SeizFt methodology and its promising potential for practical application in\\nseizure detection.\\nNotably, the above-described comparative results illustrate the importance of the\\nSeizFt employed Fourier-transform surrogates to address prior limitations of deep learn-\\ning. Through the randomization of Fourier-transform phases of temporal-spatial data, FT\\nSurrogates are generated that provide a means to balance the dataset for EEG-based seizure\\ndetection. The technique utilizes these surrogates to augment underrepresented classes,\\nthereby achieving a more balanced dataset for training subsequent models. Evidence of the\\nimpact of the FT Surrogate method can be seen in the improvement of Task 2, as outlined\\nin Table 2. Our proposed approach employs the same ChronoNet model as the organizer\\nBenchmark. However, our approach adds class re-balancing and FT Surrogates. Due to\\nthe addition of these surrogates, SeizFt achieved an approximately threefold improve-\\nment (shown in line 2 of Table 2) over the organizer Benchmark model (shown in line 1\\nof Table 2).\\n4.2. Model Interpretation\\nTheSeizFt model’s decision-making process relies on several key features. The top\\nten features are visually represented in Figure 3, as identiﬁed by the SHAP method [ 64].\\nThese results demonstrate the model’s ability to discern the inﬂuence of various frequency\\nbands in an EEG signal on seizure detection. Notably, the examination of features from\\nBioengineering 2023 ,10, 918 11 of 16\\nthe frequency bands in an EEG signal is a common method for identifying correlations\\nindicative of seizures [65].\\nFigure 3. The ten most impactful features according to SHAP for SeizFt [64].\\nThe foremost inﬂuential feature, representative of the lower delta band within a\\nfrequency range of 0.4 to 1 Hz, reveals an intriguing correlation: decreased values within\\nthis band could enhance the detection of a seizure. Conversely, the feature associated\\nwith the theta band, which spans 4–8 Hz, reveals an inverse relationship. Higher values\\nwithin this band seem to decrease the likelihood of a seizure prediction by our model. The\\nsixth feature, which highlights the power ratio of the delta band to the theta band, offers\\na compelling insight: a reduction in this ratio seems to boost the effectiveness of seizure\\ndetection. These observations concur with the ﬁndings by Shoeb et al. [ 15], who established\\na correlation between higher frequencies and seizure epochs.\\nIn accordance with previous research [ 66], seizure epochs display higher values of\\nboth the Interquartile Range and Standard Deviation as compared to non-seizure epochs.\\nEntropy, however, shows similar values across seizure and non-seizure epochs. These\\nobservations are conﬁrmed by the 2nd,3rd, and 7thmost impactful features depicted in\\nFigure 3. Adding to these insights, Greene et al. [ 67] suggest that the Total Absolute Power\\nexhibits an increased value during seizure epochs compared to non-seizure epochs. This\\nobservation is supported by the 4th most signiﬁcant feature and further corroborated by\\nthe 9th feature that amalgamates the power in both Delta and Theta bands.\\nThese ﬁndings reinforce the ability of the SeizFt model to distinguish accurately\\nbetween seizure and non-seizure epochs not only in the frequency domain of EEG signals\\nbut also in the time domain. This distinction further conﬁrms the potential utility of SeizFt\\nin real-world seizure detection applications.\\n4.3. Model Robustness and Generalizability\\nThe performance of SeizFt on the SeizeIT2 dataset [ 47] underscores the robust na-\\nture of the model in recognizing seizures from wearable SensorDot data. The model’s\\ndesign incorporates data augmentation and class balancing strategies, which improve its\\nresilience to variations and anomalies in EEG signals and bolsters its capacity to general-\\nBioengineering 2023 ,10, 918 12 of 16\\nize to unseen data. Moreover, SeizFt utilizes an ensemble of decision trees through the\\nCatBoost classiﬁer [43,44], enhancing the model’s stability and accuracy. This strategic\\nchoice enables the model to provide reliable and accurate predictions for deployment in\\nclinical scenarios. As such, the development of real-time, on-device seizure prediction\\nalgorithms using the SeizFt framework could enable proactive epilepsy management and\\nearly intervention strategies.\\n5. Discussion\\nIn this study, we have presented SeizFt , an interpretable seizure detection framework\\nthat combines robust feature extraction with an effective augmentation strategy. The main\\ncontributions of this study are as follows:\\n• We propose a deep neural network that incorporates CNN, LSTM, and multi-headed\\nattention elements, which outperforms other deep learning techniques.\\n• We introduce SeizFt , a robust and interpretable framework for seizure detection\\nin wearable EEGs, which amalgamates feature extraction, data augmentation via\\nFourier Transform (FT) Surrogates, class balancing, and a CatBoost-driven ensemble\\nof decision trees.\\n• Through experimental evaluation, we illustrate the superior performance of our\\nSeizFt framework in terms of sensitivity and false alarm rates relative to existing\\nstate-of-the-art methods.\\n• We elucidate the crucial, clinically interpretable features employed by the SeizFt\\nframework that characterize seizures as measured via EEG. The SeizFt features\\nenhance trust in the model’s predictive capability.\\n5.1. Interpretable Models Can Compete with Black-Box Models\\nTraditionally, deep learning “black-box” models have outperformed their “glass-box”\\nor interpretable model counterparts when it comes to predictive accuracy. However, the\\nlack of interpretability of black-box models to explain their predictions lessens trust in\\ntheir use for high-stakes healthcare decision support. Ideally, healthcare applications need\\nblack-box model accuracy with glass-box model interpretability. Our approach with SeizFt\\ndemonstrates that interpretable models can, in fact, outperform black-box deep learning\\nmethods for detecting rare events in physiological monitoring. SeizFt was able to eclipse\\nstate-of-the-art black-box model accuracy for seizure detection and minimize false alarms.\\nMoreover, SeizFt maintains transparent model interpretability using well-established\\nfeatures trusted by practicing clinicians.\\n5.2. Evaluation of Design Choices That Made SeizFt a Success\\nWith its accuracy besting state-of-the-art deep learning models and its feature-based\\ninterpretability, SeizFt becomes a new benchmark for automated seizure detection using\\nwearable EEG devices. The success of SeizFt can be attributed to several key factors.\\nFirstly, the use of Fourier Transform (FT) Surrogates for data augmentation addresses\\nthe challenge of class imbalance, which is commonly encountered in seizure detection\\ntasks. By generating synthetic seizure and non-seizure epochs, our approach ensures a\\nbalanced representation of both classes during the training process. The achieved class\\nbalance results in a more accurate and generalizable model.\\nSecondly, SeizFt employs a diverse set of robust features inspired by recent work in\\ninterpretable sleep staging [ 39]. These features, such as standard deviation, interquartile\\nrange, skewness, kurtosis, and power in different energy bands, enable the model to capture\\nthe complex characteristics of EEG signals associated with seizure events. This, in turn,\\nallows for more accurate classiﬁcation of seizure and non-seizure epochs.\\nFinally, the model design choice of employing an ensemble of trees using CatBoost\\nas the classiﬁer plays a crucial role in the effectiveness of SeizFt . The use of an ensemble\\napproach provides increased stability and accuracy in the model, which is essential for its\\nadoption in clinical settings where reliability is of utmost importance.\\nBioengineering 2023 ,10, 918 13 of 16\\n5.3. Potential for Clinical Impact\\nThe proposed SeizFt framework could markedly revolutionize the clinical manage-\\nment of epilepsy, offering the capability for precise, real-time seizure detection through\\nwearable devices. Unlike many black-box machine learning models, SeizFt is an in-\\nterpretable model. As such, SeizFt fosters trust and transparency in its use in clinical\\nsettings [35,37,38] . More speciﬁcally, the model bases its predictions on concrete and tan-\\ngible features. As such, the likelihood of false positives and negatives is reduced, which\\nincreases overall reliability. With this framework, healthcare professionals can promptly\\nintervene during an epileptic seizure and adjust treatment plans in a personalized manner.\\nThus, SeizFt has the potential to signiﬁcantly improve patient outcomes and enhance the\\nquality of life for individuals living with epilepsy.\\n5.4. Limitations and Future Directions\\nSeizFt has shown impressive performance in seizure detection through wearable de-\\nvices, outperforming state-of-the-art benchmarks by \\x1830%. Although the method demon-\\nstrates adaptability to various datasets and a broad range of applications, further trials\\non more diverse epilepsy datasets and with different wearable EEG sensors would pro-\\nvide a more comprehensive assessment of the method’s efﬁciency. There is potential for\\nfuture research to incorporate additional physiological indicators known to ﬂuctuate be-\\nfore, during, and after a seizure. Features such as heart rate variability, motion data, and\\nperspiration [65] could potentially augment the model’s performance in real time. As the\\ntechnologies in integrated sensors advance, additional features such as changes in envi-\\nronmental light and sound could be considered, given their known potential to precipitate\\nseizures in some individuals.\\n6. Conclusions\\nIn conclusion, our proposed SeizFt framework represents a signiﬁcant advancement\\nin the ﬁeld of seizure detection. By leveraging a combination of robust feature extraction,\\ndata augmentation, and class balancing strategies, SeizFt demonstrates superior perfor-\\nmance compared to other state-of-the-art approaches, including black-box deep learning\\nmethods. The interpretability of SeizFt is a key advantage, as it fosters trust and account-\\nability among healthcare professionals who rely on the model to make critical decisions\\nregarding the diagnosis and management of epilepsy. Moreover, the successful application\\nofSeizFt to wearable SensorDot data suggests its potential for real-time, continuous mon-\\nitoring of brain activity to improve patient diagnostics, management, intervention, and\\nquality of life via personalized medicine.\\nAuthor Contributions: Conceptualization, I.A.-H. and C.S.M.; methodology, I.A.-H.; software, I.A.-\\nH.; validation, I.A.-H. and C.S.M; formal analysis, I.A.-H.; investigation, I.A.-H. and C.S.M.; re-\\nsources, C.S.M.; data processing, I.A.-H.; writing—original draft preparation, I.A.-H. and C.S.M.;\\nwriting—review and editing, I.A.-H. and C.S.M.; visualization, I.A.-H.; supervision, C.S.M.; project\\nadministration, C.S.M.; funding acquisition, C.S.M. All authors have read and agreed to the published\\nversion of the manuscript.\\nFunding: This research was funded by the National Science Foundation CAREER grant 1944247\\nto C.S.M, the National Institute of Health grant U19-AG056169 sub-award to C.S.M., the Chan\\nZuckerberg Initiative grant 253558 to C.S.M.\\nInstitutional Review Board Statement: Not applicable.\\nInformed Consent Statement: Not Applicable.\\nData Availability Statement: The code for SeizFt is freely available on GitHub at: https://github.c\\nom/iah3/interpretable-seizure (accessed on 30 July 2023). Data were made available to participants of\\nthe 2023 ICASSP Seizure Detection Grand Challenge. Requests for data should be made to the Grand\\nChallenge organizers: https://biomedepi.github.io/seizure_detection_challenge/team/ (accessed\\non 30 July 2023).\\nBioengineering 2023 ,10, 918 14 of 16\\nConﬂicts of Interest: The authors declare no conﬂict of interest. The funders had no role in the design\\nof the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or\\nin the decision to publish the results.\\nReferences\\n1. Paz, J.T.; Huguenard, J.R. Microcircuits and their interactions in epilepsy: Is the focus out of focus? Nat. Neurosci. 2015 ,18, 351–359.\\n[CrossRef]\\n2. Staley, K. Molecular mechanisms of epilepsy. Nat. Neurosci. 2015 ,18, 367–372. [CrossRef] [PubMed]\\n3. Jirsa, V .K.; Stacey, W.C.; Quilichini, P .P .; Ivanov, A.I.; Bernard, C. On the nature of seizure dynamics. Brain 2014 ,137, 2210–2230.\\n[CrossRef]\\n4. Hauser, W.A. Seizure disorders: The changes with age. Epilepsia 1992 ,33, 6–14. [CrossRef] [PubMed]\\n5. Jory, C.; Shankar, R.; Coker, D.; McLean, B.; Hanna, J.; Newman, C. Safe and sound? A systematic literature review of seizure\\ndetection methods for personal use. Seizure 2016 ,36, 4–15. [CrossRef]\\n6. Gómez, C.; Arbeláez, P .; Navarrete, M.; Alvarado-Rojas, C.; Le Van Quyen, M.; Valderrama, M. Automatic seizure detection\\nbased on imaged-EEG signals through fully convolutional networks. Sci. Rep. 2020 ,10, 21833. [CrossRef]\\n7. Wang, Z.; Mengoni, P . Seizure classiﬁcation with selected frequency bands and EEG montages: A Natural Language Processing\\napproach. Brain Inform. 2022 ,9, 11. [CrossRef] [PubMed]\\n8. Yan, X.; Yang, D.; Lin, Z.; Vucetic, B. Signiﬁcant low-dimensional spectral-temporal features for seizure detection. IEEE Trans.\\nNeural Syst. Rehabil. Eng. 2022 ,30, 668–677. [CrossRef] [PubMed]\\n9. Thompson, M.E.; Langer, J.; Kinfe, M. Seizure detection watch improves quality of life for adolescents and their families. Epilepsy\\nBehav. 2019 ,98, 188–194. [CrossRef]\\n10. Chiang, S.; Moss, R.; Patel, A.D.; Rao, V .R. Seizure detection devices and health-related quality of life: A patient-and caregiver-\\ncentered evaluation. Epilepsy Behav. 2020 ,105, 106963. [CrossRef]\\n11. Zhao, X.; Lhatoo, S.D. Seizure detection: Do current devices work? And when can they be useful? Curr. Neurol. Neurosci. Rep.\\n2018 ,18, 1–19. [CrossRef]\\n12. Pohlmann-Eden, B.; Newton, M. First seizure: EEG and neuroimaging following an epileptic seizure. Epilepsia 2008 ,49, 19–25.\\n[CrossRef] [PubMed]\\n13. Shellhaas, R.A. Continuous long-term electroencephalography: The gold standard for neonatal seizure diagnosis. In Seminars in\\nFetal and Neonatal Medicine ; Elsevier: Piscataway, NJ, USA, 2015; Volume 20, pp. 149–153.\\n14. Haider, H.A.; Esteller, R.; Hahn, C.D.; Westover, M.B.; Halford, J.J.; Lee, J.W.; Shaﬁ, M.M.; Gaspard, N.; Herman, S.T.; Gerard, E.E.;\\net al. Sensitivity of quantitative EEG for seizure identiﬁcation in the intensive care unit. Neurology 2016 ,87, 935–944. [CrossRef]\\n[PubMed]\\n15. Shoeb, A.H.; Guttag, J.V . Application of machine learning to epileptic seizure detection. In Proceedings of the 27th International\\nConference on Machine Learning (ICML-10), Haifa, Israel, 21–24 June 2010; pp. 975–982.\\n16. Shoeb, A.H. Application of Machine Learning to Epileptic Seizure Onset Detection and Treatment. Ph.D. Thesis, Massachusetts\\nInstitute of Technology, Cambridge, MA, USA, 2009.\\n17. Athena, F.F.; West, M.P .; Hah, J.; Hanus, R.; Graham, S.; Vogel, E.M. Towards a better understanding of the forming and resistive\\nswitching behavior of Ti-doped HfOx RRAM. J. Mater. Chem. C 2022 ,10, 5896–5904. [CrossRef]\\n18. Gong, N.; Rasch, M.; Seo, S.C.; Gasasira, A.; Solomon, P .; Bragaglia, V .; Consiglio, S.; Higuchi, H.; Park, C.; Brew, K.; et al. Deep\\nlearning acceleration in 14 nm CMOS compatible ReRAM array: Device, material and algorithm co-optimization. In Proceedings\\nof the IEEE International Electron Devices Meeting, San Francisco, CA, USA, 3–7 December 2022.\\n19. Basnet, P .; Anderson, E.C.; Athena, F.F.; Chakrabarti, B.; West, M.P .; Vogel, E.M. Asymmetric Resistive Switching of Bilayer\\nHfOx/AlOy and AlOy/HfOx Memristors: The Oxide Layer Characteristics and Performance Optimization for Digital Set and\\nAnalog Reset Switching. ACS Appl. Electron. Mater. 2023 ,5, 1859–1865. [CrossRef]\\n20. Hah, J.; West, M.P .; Athena, F.F.; Hanus, R.; Vogel, E.M.; Graham, S. Impact of oxygen concentration at the HfOx/Ti interface on\\nthe behavior of HfOx ﬁlamentary memristors. J. Mater. Sci. 2022 ,57, 9299–9311. [CrossRef]\\n21. Athena, F.F.; West, M.P .; Hah, J.; Graham, S.; Vogel, E.M. Trade-Off between Gradual Set and On/Off Ratio in HfOx-Based Analog\\nMemory with a Thin SiOx Barrier Layer. ACS Appl. Electron. Mater. 2023 ,5, 3048–3058. [CrossRef]\\n22. Zhang, J.; Chatzichristos, C.; Vandecasteele, K.; Swinnen, L.; Broux, V .; Cleeren, E.; Van Paesschen, W.; De Vos, M. Automatic\\nannotation correction for wearable EEG based epileptic seizure detection. J. Neural Eng. 2022 ,19, 016038. [CrossRef]\\n23. Brinkmann, B.H.; Karoly, P .J.; Nurse, E.S.; Dumanis, S.B.; Nasseri, M.; Viana, P .F.; Schulze-Bonhage, A.; Freestone, D.R.; Worrell,\\nG.; Richardson, M.P .; et al. Seizure diaries and forecasting with wearables: Epilepsy monitoring outside the clinic. Front. Neurol.\\n2021 ,12, 690404. [CrossRef]\\n24. Gu, Y.; Cleeren, E.; Dan, J.; Claes, K.; Van Paesschen, W.; Van Huffel, S.; Hunyadi, B. Comparison between scalp EEG and\\nbehind-the-ear EEG for development of a wearable seizure detection system for patients with focal epilepsy. Sensors 2017 ,18, 29.\\n[CrossRef]\\n25. Olokodana, I.L.; Mohanty, S.P .; Kougianos, E.; Sherratt, R.S. EZcap: A novel wearable for real-time automated seizure detection\\nfrom EEG signals. IEEE Trans. Consum. Electron. 2021 ,67, 166–175. [CrossRef]\\nBioengineering 2023 ,10, 918 15 of 16\\n26. Thodoroff, P .; Pineau, J.; Lim, A. Learning robust features using deep learning for automatic seizure detection. In Proceedings of\\nthe 1st Machine Learning for Healthcare Conference, PMLR, Los Angeles, CA, USA, 19–20 August 2016; pp. 178–190.\\n27. Yuan, Y.; Xun, G.; Jia, K.; Zhang, A. A multi-view deep learning framework for EEG seizure detection. IEEE J. Biomed. Health\\nInform. 2018 ,23, 83–94. [CrossRef] [PubMed]\\n28. Hossain, M.S.; Amin, S.U.; Alsulaiman, M.; Muhammad, G. Applying deep learning for epilepsy seizure detection and brain\\nmapping visualization. ACM Trans. Multimed. Comput. Commun. Appl. (TOMM) 2019 ,15, 1–17. [CrossRef]\\n29. Gramacki, A.; Gramacki, J. A deep learning framework for epileptic seizure detection based on neonatal EEG signals. Sci. Rep.\\n2022 ,12, 13010. [CrossRef] [PubMed]\\n30. Cherian, R.; Kanaga, E.G. Theoretical and methodological analysis of EEG based seizure detection and prediction: An exhaustive\\nreview. J. Neurosci. Methods 2022 ,369, 109483. [CrossRef]\\n31. Al-Hussaini, I.; Xiao, C.; Westover, M.B.; Sun, J. SLEEPER: Interpretable Sleep staging via Prototypes from Expert Rules. In\\nProceedings of the Machine Learning for Healthcare Conference, PMLR, Ann Arbor, MI, USA, 9–10 August 2019; pp. 721–739.\\n32. Kaushik, G.; Gaur, P .; Sharma, R.R.; Pachori, R.B. EEG signal based seizure detection focused on Hjorth parameters from\\ntunable-Q wavelet sub-bands. Biomed. Signal Process. Control 2022 ,76, 103645. [CrossRef]\\n33. Shen, M.; Wen, P .; Song, B.; Li, Y. An EEG based real-time epilepsy seizure detection approach using discrete wavelet transform\\nand machine learning methods. Biomed. Signal Process. Control 2022 ,77, 103820. [CrossRef]\\n34. Zhang, Y.; Yao, S.; Yang, R.; Liu, X.; Qiu, W.; Han, L.; Zhou, W.; Shang, W. Epileptic seizure detection based on bidirectional gated\\nrecurrent unit network. IEEE Trans. Neural Syst. Rehabil. Eng. 2022 ,30, 135–145. [CrossRef]\\n35. Doshi-Velez, F.; Kim, B. Considerations for evaluation and generalization in interpretable machine learning. In Explainable and\\nInterpretable Models in Computer Vision and Machine Learning ; Springer: Berlin/Heidelberg, Germany, 2018; pp. 3–17.\\n36. Al-Hussaini, I.; Mitchell, C.S. SERF: Interpretable Sleep Staging using Embeddings, Rules, and Features. In Proceedings of\\nthe 31st ACM International Conference on Information & Knowledge Management, Atlanta, GA, USA, 17–21 October 2022;\\npp. 3791–3795.\\n37. Hooker, S.; Erhan, D.; Kindermans, P .J.; Kim, B. A benchmark for interpretability methods in deep neural networks. Adv. Neural\\nInf. Process. Syst. 2019 ,32, 9737–9748.\\n38. Lipton, Z.C. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and\\nslippery. Queue 2018 ,16, 31–57. [CrossRef]\\n39. Al-Hussaini, I.; Mitchell, C.S. Performance and utility trade-off in interpretable sleep staging. In Proceedings of the NeurIPS 2022\\nWorkshop on Learning from Time Series for Health, New Orleans, LA, USA, 2 December 2022.\\n40. Van Der Donckt, J.; Van Der Donckt, J.; Deprost, E.; Vandenbussche, N.; Rademaker, M.; Vandewiele, G.; Van Hoecke, S. Do not\\nsleep on traditional machine learning: Simple and interpretable techniques are competitive to deep learning for sleep scoring.\\nBiomed. Signal Process. Control 2023 ,81, 104429. [CrossRef]\\n41. Schwabedal, J.T.; Snyder, J.C.; Cakmak, A.; Nemati, S.; Clifford, G.D. Addressing class imbalance in classiﬁcation problems of\\nnoisy signals by using fourier transform surrogates. arXiv 2018 , arXiv:1806.08675.\\n42. Schirrmeister, R.T.; Springenberg, J.T.; Fiederer, L.D.J.; Glasstetter, M.; Eggensperger, K.; Tangermann, M.; Hutter, F.; Burgard,\\nW.; Ball, T. Deep learning with convolutional neural networks for EEG decoding and visualization. Hum. Brain Mapp. 2017 ,\\n38, 5391–5420. [CrossRef]\\n43. Dorogush, A.V .; Ershov, V .; Gulin, A. CatBoost: Gradient boosting with categorical features support. arXiv 2018 , arXiv:1810.11363.\\n44. Prokhorenkova, L.; Gusev, G.; Vorobev, A.; Dorogush, A.V .; Gulin, A. CatBoost: Unbiased boosting with categorical features. Adv.\\nNeural Inf. Process. Syst. 2018 ,31, 6639–6649.\\n45. Roy, S.; Kiral-Kornek, I.; Harrer, S. ChronoNet: A deep recurrent neural network for abnormal EEG identiﬁcation. In Artiﬁcial\\nIntelligence in Medicine, Proceedings of the 17th Conference on Artiﬁcial Intelligence in Medicine, AIME 2019, Proceedings 17, Poznan,\\nPoland, 26–29 June 2019 ; Springer: Berlin/Heidelberg, Germany, 2019; pp. 47–56.\\n46. Al-Hussaini, I.; Mitchell, C.S. Towards Interpretable Seizure Detection Using Wearables. In Proceedings of the ICASSP 2023—2023\\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 4–10 June 2023.\\n[CrossRef]\\n47. Chatzichristos, C.; Swinnen, L.; Macea, J.; Bhagubai, M.; Van Paesschen, W.; De Vos, M. Multimodal detection of typical absence\\nseizures in home environment with wearable electrodes. Front. Signal Process. 2022 ,2, 1014700. [CrossRef]\\n48. Chatzichristos, C.; Claro Bhagubai, M.; Van Paesschen, W.; De Vos, M. SeizeIT1 ; KU Leuven RDR: Leuven, Belgium, 2023.\\n49. Shah, V .; Golmohammadi, M.; Obeid, I.; Picone, J. Objective evaluation metrics for automatic classiﬁcation of EEG events. In\\nBiomedical Signal Processing: Innovation and Applications ; Springer: Cham, Switzerland, 2021; pp. 223–255.\\n50. Zhong, Z.; Zheng, L.; Kang, G.; Li, S.; Yang, Y. Random erasing data augmentation. In Proceedings of the AAAI Conference on\\nArtiﬁcial Intelligence, New York, NY, USA, 7–12 February 2020; Volume 34, pp. 13001–13008.\\n51. Xie, Q.; Dai, Z.; Hovy, E.; Luong, T.; Le, Q. Unsupervised data augmentation for consistency training. Adv. Neural Inf. Process.\\nSyst. 2020 ,33, 6256–6268.\\n52. Cubuk, E.D.; Zoph, B.; Shlens, J.; Le, Q.V . Randaugment: Practical automated data augmentation with a reduced search space. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, Seattle, WA, USA, 14–19\\nJune 2020; pp. 702–703.\\n53. Ross, C.L. Energy medicine: Current status and future perspectives. Glob. Adv. Health Med. 2019 ,8, 2164956119831221. [CrossRef]\\nBioengineering 2023 ,10, 918 16 of 16\\n54. Gramfort, A.; Luessi, M.; Larson, E.; Engemann, D.; Strohmeier, D.; Brodbeck, C.; Goj, R.; Jas, M.; Brooks, T.; Parkkonen, L.; et al.\\nMEG and EEG data analysis with MNE-Python. Front. Neurosci. 2013 ,7, 267. [CrossRef]\\n55. Van Der Donckt, J.; Van Der Donckt, J.; Deprost, E.; Van Hoecke, S. tsﬂex: Flexible time series processing & feature extraction.\\nSoftwareX 2021 ,17, 100971.\\n56. Virtanen, P .; Gommers, R.; Oliphant, T.E.; Haberland, M.; Reddy, T.; Cournapeau, D.; Burovski, E.; Peterson, P .; Weckesser,\\nW.; Bright, J.; et al. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python. Nat. Methods 2020 ,17, 261–272.\\n[CrossRef] [PubMed]\\n57. Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V .; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P .; Weiss, R.; Dubourg, V .;\\net al. Scikit-learn: Machine Learning in Python. J. Mach. Learn. Res. 2011 ,12, 2825–2830.\\n58. Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; et al. Pytorch:\\nAn imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 2019 ,32, 8026–8037.\\n59. Abadi, M.; Agarwal, A.; Barham, P .; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.; Devin, M.; et al. TensorFlow:\\nLarge-Scale Machine Learning on Heterogeneous Systems. 2015. Available online: www.tensorflow.org (accessed on 30 July 2023).\\n60. Panchavati, S.; Dussen, S.V .; Semwal, H.; Ali, A.; Chen, J.; Li, H.; Arnold, C.; Speier, W. Pretrained Transformers for Seizure\\nDetection. In Proceedings of the ICASSP 2023—2023 IEEE International Conference on Acoustics, Speech and Signal Processing\\n(ICASSP), Rhodes Island, Greece, 4–10 June 2023. [CrossRef]\\n61. Shah, V .; Von Weltin, E.; Lopez, S.; McHugh, J.R.; Veloso, L.; Golmohammadi, M.; Obeid, I.; Picone, J. The temple university\\nhospital seizure detection corpus. Front. Neuroinform. 2018 ,12, 83. [CrossRef] [PubMed]\\n62. Huang, B.; Abtahi, A.; Aminifar, A. Lightweight Machine Learning for Seizure Detection on Wearable Devices. In Proceedings of\\nthe ICASSP 2023—2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island,\\nGreece, 4–10 June 2023. [CrossRef]\\n63. Shovkun, A.; Kiryasov, A.; Zakharov, I.; Khayretdinova, M. Optimization of the Deep Neural Networks for Seizure Detection. In\\nProceedings of the ICASSP 2023—2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\\nRhodes Island, Greece, 4–10 June 2023. [CrossRef]\\n64. Lundberg, S.M.; Lee, S.I. A uniﬁed approach to interpreting model predictions. Adv. Neural Inf. Process. Syst. 2017 ,30, 4768–4777.\\n65. Marchal, R.; Rheims, S. Assessing epilepsy-related autonomic manifestations: Beyond cardiac and respiratory investigations.\\nNeurophysiol. Clin. 2023 ,53, 102850. [CrossRef]\\n66. Bedeeuzzaman, M.; Farooq, O.; Khan, Y.U. Automatic seizure detection using inter quartile range. Int. J. Comput. Appl. 2012 ,\\n44, 1–5. [CrossRef]\\n67. Greene, B.R.; Faul, S.; Marnane, W.; Lightbody, G.; Korotchikova, I.; Boylan, G.B. A comparison of quantitative EEG features for\\nneonatal seizure detection. Clin. Neurophysiol. 2008 ,119, 1248–1261. [CrossRef]\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = Path(\"./data/included_pdf/pdf.pdf\")\n",
    "extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
