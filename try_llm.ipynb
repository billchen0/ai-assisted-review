{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from model_client import ModelClient, get_prompt\n",
    "from pdf_extraction import extract_text_from_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pdfs/s41598-023-34517-w.pdf\n",
      "www.nature.com/scientificreports\n",
      "OPEN Mental stress recognition on the fly\n",
      "using neuroplasticity spiking neural\n",
      "networks\n",
      "Mahima Milinda Alwis Weerasinghe 1,2*, Grace Wang 3,4, Jacqueline Whalley 5 &\n",
      "Mark Crook‑Rumsey 6,7\n",
      "Mental stress is found to be strongly connected with human cognition and wellbeing. As the\n",
      "complexities of human life increase, the effects of mental stress have impacted human health and\n",
      "cognitive performance across the globe. This highlights the need for effective non‑invasive stress\n",
      "detection methods. In this work, we introduce a novel, artificial spiking neural network model called\n",
      "Online Neuroplasticity Spiking Neural Network (O‑NSNN) that utilizes a repertoire of learning\n",
      "concepts inspired by the brain to classify mental stress using Electroencephalogram (EEG) data.\n",
      "These models are personalized and tested on EEG data recorded during sessions in which participants\n",
      "listen to different types of audio comments designed to induce acute stress. Our O‑NSNN models\n",
      "learn on the fly producing an average accuracy of 90.76% (σ = 2.09) when classifying EEG signals\n",
      "of brain states associated with these audio comments. The brain‑inspired nature of the individual\n",
      "models makes them robust and efficient and has the potential to be integrated into wearable\n",
      "technology. Furthermore, this article presents an exploratory analysis of trained O‑NSNNs to discover\n",
      "links between perceived and acute mental stress. The O‑NSNN algorithm proved to be better for\n",
      "personalized stress recognition in terms of accuracy, efficiency, and model interpretability.\n",
      "Implications of mental stress. We often encounter stress in daily life with variations of intensity and\n",
      "prolongation. Stress is understood as the response of the human body to mental and/or physical stimuli that\n",
      "involves the nervous system and hypothalamus-pituitary-adrenocortical axis1. According to the literature, stress\n",
      "is often classified as acute, episodic, or c hronic2. Many contemporary studies have found stress to have a major\n",
      "impact on human health and cognitive performance. In some cases, stress has been shown to have a direct con-\n",
      "nection to depression, anxiety, stroke, cardiovascular disease, cancer, speech, and cognition impairment3–5. The\n",
      "negative effects of stress on human cognition are associated with dysfunctional changes in the prefrontal cortex\n",
      "and amygdala a ctivation5,6, whereas the physical health effects of stress are related to detrimental changes in\n",
      "immunity and physical h omeostasis7. As the complexities of human life increase, the effects of stress have begun\n",
      "to burden nations and, the globe at large2,8, which highlights the requirement for more research in this area. Early\n",
      "detection of harmful stress can be crucial as a part of effective stress management to promote greater wellbeing.\n",
      "Stress and electroencephalogram. Rapid development in sensor technologies and machine learning\n",
      "(ML) techniques have enabled research communities to begin to develop automated stress detection systems.\n",
      "These systems use invasive and/or non-invasive data acquisition methods. Stress recognition using invasive\n",
      "methods can be highly time-consuming and often require experts for data acquisition and processing9–11; this\n",
      "is not ideal for an automated system. The most common non-invasive methods include Electroencephalo-\n",
      "gram (EEG), heart rate variability, galvanic skin response, blood volume pulse, and electromyography for data\n",
      "1School of Engineering, Computer and Mathematical Sciences, Auckland University of Technology, Auckland,\n",
      "New Zealand. 2Brain-Inspired AI and Neuroinformatics Lab, Department of Data Science, Sri Lanka Technological\n",
      "Campus, Padukka, Sri Lanka. 3School of Psychology and Wellbeing, University of Southern Queensland,\n",
      "Toowoomba, Australia. 4Centre for Health Research, University of Southern Queensland, Toowoomba,\n",
      "Australia. 5Department of Computer Science and Software Engineering, Auckland University of Technology,\n",
      "Auckland, New Zealand. 6Department of Basic and Clinical Neuroscience, King’s College London, London,\n",
      "UK. 7UK Dementia Research Institute, Centre for Care Research and Technology, Imperial College London, London,\n",
      "UK. *email: mahimarcrc@gmail.com\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 1\n",
      "Vol.:(0123456789)\n",
      "www.nature.com/scientificreports/\n",
      "acquisition12. Of these non-invasive methods, EEG is used most extensively for stress recognition due to its:\n",
      "information richness, cost-effectiveness, and high temporal resolution13.\n",
      "Stress recognition on the fly. Current methods for stress recognition use traditional ML techniques such\n",
      "as Linear Discriminant Analysis14, Naive-Bayes15, Support Vector Machine16, K-Nearest N eighbor17, and Multi-\n",
      "layer Perceptron18. However, these methods are not capable of evolving and adapting to new information after\n",
      "training, preventing them from being used in an online s etup19. Online learning typically uses real-world data\n",
      "that changes with time, thus the model is adaptive and learns as new data is fed into it over time. In contrast,\n",
      "most stress detection approaches presented in the literature use static data to train and test the model. They also\n",
      "typically employ interventions, to manipulate the data used to train and test the models, such as feature engi-\n",
      "neering methods. It is difficult to compare the performance of known stress detection models because the feature\n",
      "engineering and extraction approaches differ from one study to another. This lack of standards also means that\n",
      "the generalizability of the methods presented is questionable20. Moreover, these traditional methods require a\n",
      "high volume of labelled data for model training. Today, the emergence of wearable technologies has revealed\n",
      "the potential for personalized health applications, designed to detect stress. Such applications must meet certain\n",
      "conditions to be practical. Use of online learning to allow the model to adapt to change, capability to operate\n",
      "under low power and the need for low-resource utilization are among them. This work focuses on finding solu-\n",
      "tions for the challenges posed by these conditions.\n",
      "Data drifts and online learning. One of the challenges in online learning is handling what is known as\n",
      "the drift phenomena successfully. Drifts can be observed in spatiotemporal data such as EEG and they can be\n",
      "defined in terms of input(s) and concept(s)21. The input(s) drift refers to the change of input data distribution\n",
      "over time without affecting the posterior probabilities of classes; concept drift refers to the change of poste-\n",
      "rior probabilities of the classes over time without any changes in the input d istribution22. The drift phenomena\n",
      "require ML techniques to be able to acquire new knowledge without forgetting the prior knowledge (i.e., avoid-\n",
      "ing catastrophic forgetting) and even to update prior knowledge based on that new or recently gained knowl-\n",
      "edge. Adding to the challenge are the restrictions posed by online learning which demands the algorithm to use\n",
      "only a limited amount of pre-allocated memory, process a sample only once, use a consistent amount of time\n",
      "for processing, produce a valid model at each processing step, and perform in par with batch mode l earning19.\n",
      "Spiking neural networks (SNNs). SNNs are a class of artificial neural networks (ANNs) that are consid-\n",
      "ered to be biologically p lausible23. They have proven to be highly efficient in terms of time and memory require-\n",
      "ments for data processing compared to commonly used sigmoidal counter parts23. The temporal dimension used\n",
      "in data processing is a major factor that contributes to their increased efficiency when compared with traditional\n",
      "ANNs, which makes SNNs an ideal candidate for online l earning24. Moreover, the unsupervised learning mecha-\n",
      "nisms in SNNs have demonstrated capability in fast and data-efficient l earning25–27. These attributes have led\n",
      "to the development of several online learning algorithms using SNNs with both supervised and unsupervised\n",
      "learning21,28–37. Of these methods, only a few algorithms use structural adaptation (i.e., evolving and pruning\n",
      "neurons and connections). Structural adaptation is crucial for learning new knowledge and forgetting irrelevant\n",
      "information in an online setup21,29,34,35,37. However, some of these structurally adaptive methods are built for\n",
      "batch mode learning only29,37 or do not fully exploit the temporal dynamics through learning21,34,35.\n",
      "The online neuroplasticity spiking neural network (O‑NSNN). The O-NSNN introduced in this work uses math-\n",
      "ematical abstractions of selected plasticity techniques found in brain functions to fully exploit spatiotemporal\n",
      "patterns present in the data. This does not mean that the model mimics the entire neurobiological process of\n",
      "the brain, but rather it uses selected concepts of signal encoding, propagating, processing, and learning found\n",
      "in the brain. This algorithm differs from the previous ASNNs21,29,34,35,37 due to the inclusion of a full repertoire\n",
      "of plasticity techniques for temporal learning. These techniques are Spike Time Dependent Plasticity (STDP)38,\n",
      "Intrinsic Plasticity(IP)39, Neuron Evolving (neuron addition)40 and Neuron Pruning (neuron elimination)41. We\n",
      "hypothesize that this algorithm (see Fig. 1) will produce stable and faster pattern separation capability in the\n",
      "online classification of stress-related EEG by considering and handling the challenges associated with online\n",
      "learning.\n",
      "The proposed O-NSNN consists of three layers of Leaky-Integrate and Fire neurons (LIF)42 (see Fig. 2); a\n",
      "mathematical abstraction of a biological neuron that has demonstrated a greater balance between biological\n",
      "plausibility and computational tractability43. Before processing, the EEG signals are converted to their spiking\n",
      "equivalent using Address Event Representation (AER); a spike encoding algorithm used in artificial retina44.\n",
      "Thereafter, the first layer of neurons propagates spikes to the second layer via excitatory (blue) and inhibitory\n",
      "(black) synapses. During this propagation, the synaptic weights are updated using the STDP r ule38. In addition, all\n",
      "the neurons adjust their excitability using an IP r ule45. This combination of unsupervised STDP and IP prevents\n",
      "the network from getting caught up in a potentiation loop46, ensuring h omeostasis47 and helping neurons extract\n",
      "independent spiking features from the input48. Moreover, the second layer of neurons undergoes a self-pruning\n",
      "process induced by error monitoring to avoid misclassifications caused by low-spiking neurons45. The synapses\n",
      "from the second layer to the third layer are excitatory and, follow a similar weight updating strategy discussed\n",
      "in dynamically evolving SNN (deSNN)49 that can evolve new neurons in the presence of new knowledge. How-\n",
      "ever, unlike in deSNN, output neurons are not merged based on weight vector similarities (i.e., calculated using\n",
      "Euclidean distance of the input weight vector of a given neuron). In the presence of data drift, neurons of similar\n",
      "Euclidean distances may represent different classes. Therefore, we do not merge neurons rather, we eliminate\n",
      "or preserve neurons created based on the classification errors made during the data processing (Please refer to\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 2\n",
      "Vol:.(1234567890)\n",
      "www.nature.com/scientificreports/\n",
      "Figure 1. Flow diagram of the experiment. The experiment is conducted according to test-then-train r egime22.\n",
      "Under this regime, the network is only trained when a prediction is incorrect.\n",
      "the Methods section for an in-depth explanation). This combined process of neuron addition in the third layer\n",
      "and, neuron pruning in the second layer are unique implementations that have not been discussed together in\n",
      "the published literature, to the best of our knowledge.\n",
      "Acute stress and data collection. The dataset used in this study consists of EEG recordings from 22\n",
      "healthy participants (twelve males—average age = 27.92 years, standard deviation (σ) = 3.09 and ten females—the\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 3\n",
      "Vol.:(0123456789)\n",
      "www.nature.com/scientificreports/\n",
      "Figure 2. (a) The proposed O-NSNN architecture for stress recognition. EEG originating from FP1, FP2, T7\n",
      "and T8 channels are encoded into spikes (using the AER algorithm) and propagated through a three-layered\n",
      "SNN architecture. An STDP rule is used for temporal learning between the input layer and the hidden layer.\n",
      "Hidden layer neurons use IP to adapt excitability based on the incoming data. The output layer learns using RO\n",
      "and SDSP rules. Each hidden layer neuron prunes itself according to soft-pruning rule and, the output layer\n",
      "evolves. (b) Stress class input samples of P1 with different spike rate distribution (Input drift) (c) Two separate\n",
      "classes of P1 (Critical and Positive) with the same input spiking distributions (Concept drift).\n",
      "average age of 25.9 years, σ = 8.20) across three different conditions. On each condition, the participants were\n",
      "asked to listen to one type of comment, either critical, neutral, or positive. Such critical comments stimulate\n",
      "the part of the human auditory system of which the primary objective is to alert and w arn50. Moreover, audio\n",
      "criticism has also been shown to induce mental stress levels in previous studies51–53 and music to induce positive\n",
      "and negative e motions54. Based on these previous studies, we presumed that the critical audio comments would\n",
      "induce acute stress in the participant. The details of these comments used for this study have been validated and\n",
      "published reviosuly55,56. In addition to EEG data, the perceived stress of each participant was recorded using the\n",
      "PSS-14 scale57. Each EEG recording lasted for two minutes, and the recordings were segmented into five-second\n",
      "splits to feed the O-NSNN. Consequently, a single sample of EEG data consisted of 1280 time points and four\n",
      "channels. From each participant, 72 such samples, with 24 samples for each class of stressed, neutral, and posi-\n",
      "tive, were processed. Complete details of the dataset are given in the methods section.\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 4\n",
      "Vol:.(1234567890)\n",
      "www.nature.com/scientificreports/\n",
      "EEG channels and performance measures. For the experiments of this study, we extracted signals from the\n",
      "FP1, FP2, T7, and T8 channels. In a previous study, researchers showed the sufficiency of two frontal channels\n",
      "for stress vs non-stress classification58. Furthermore, since the stimuli were auditory, T7 and T8 channels were\n",
      "used to capture the dynamics of the auditory cortex. Classification accuracy and sensitivity (true positive rate\n",
      "for stress EEG) was used to measure the performance. These measures using O-NSNN were compared against\n",
      "70/30 split batch learning and online learning without structural plasticity (SP). For all experiments, we used\n",
      "individualized O-NSNN models since the effects of stress are found to be depending on an individual’s neu-\n",
      "robiological predisposition2. Moreover, we used the prequential accuracy metric to evaluate the performance\n",
      "of online learning59. Secondly, these individualized models were subjected to an exploratory analysis that was\n",
      "undertaken to test the interpretability of the model and see if relationships could be discovered between acute\n",
      "and participant’s perceived stress.\n",
      "This exploratory analysis involved comparing the personalized network activations to individually reported\n",
      "perceived mental stress levels. We categorized participants into one of three classes based on their PSS-14 scores\n",
      "(see Table 1). The connection weights of personalized models and Euclidean Distances (ED) of third-layer neu-\n",
      "rons were analyzed to find patterns within and between the perceived mental stress groups.\n",
      "In this work, we present a spatiotemporal data processing method for mental stress recognition and eluci-\n",
      "date the possibility of investigating brain activity at an individual level. Therefore, the contribution of this study\n",
      "benefits both computer science and psychology/neuroscience research communities. The contributions of the\n",
      "study are as follows:\n",
      "1. O-NSNN algorithm equipped with a biologically plausible repertoire of plasticity techniques for online\n",
      "mental stress recognition.\n",
      "2. Insights into how perceived stress relates to incidences of acute stress.\n",
      "Results\n",
      "We compared the classification accuracy and sensitivity of the O-NSNN model with the same learning frame-\n",
      "work without structural plasticity (SP) techniques (denoted as O-RSNN) and batch mode learning without SP\n",
      "(B-RSNN) (i.e., 70% of the samples for training and 30% for testing). The task involved measuring the accuracy\n",
      "of classifying EEG data into one of three possible classes: stress, neutral or positive conditions and the sensitiv-\n",
      "ity (true positive rate) to recognize correctly classified stress instances. Since the synaptic weights of the first\n",
      "layer to the second are initiated randomly following Gaussian distribution, each experiment was conducted 30\n",
      "times, allowing the accuracy and sensitivity to be reported statistically. The performance is presented in terms\n",
      "of average accuracy and sensitivity in Table 2. Furthermore, we explored patterns in network dynamics for\n",
      "knowledge extraction.\n",
      "Increased accuracy and robustness in O‑NSNN. The highest average accuracy for O-NSNN was\n",
      "93.63% for P1 and, the lowest was 85.29% for P18. The average accuracy across all participants was recorded at\n",
      "90.91%, 63.18% and 76.04% for O-NSNN, O-RSNN and B-RSNN, respectively, whereas the average sensitivity\n",
      "was recorded at 90.27%, 60.86% and 77.36%. The O-NSNN outperformed O-RSNN across all 22 participants.\n",
      "In comparison, B-RSNN was outperformed in terms of accuracy by O-NSNN except for one participant (P4).\n",
      "Regarding sensitivity, the B-RSNN outperformed the O-NSNN with the data of P4, P5, P7, P9 and P22.\n",
      "The performance of the O-NSNN was also compared with the most relevant studies that used a common data\n",
      "source, the DEAP dataset60, to classify stress vs relaxed brain signals (two classes). Here the O-NSNN recorded\n",
      "lower accuracy performance compared to batch mode experiments of SVM61 and SNN29 as shown in Table 3.\n",
      "Figure 3 shows the variation of performance for personalized models for each participant obtained from 30\n",
      "pseudo-random network initiations. Accordingly, for all 22 participants, the O-NSNN model had the lowest\n",
      "degree of performance variation.\n",
      "The efficiency of O‑NSNN. The efficiency factor of the O-NSNN can be presented in terms of the number\n",
      "of output neurons used and spikes generated in the hidden layer. When the number of output neurons used was\n",
      "investigated, the O-NSNN method used, on average 20.39 (σ = 3.84) neurons (see Fig. 4a), whereas O-RSNN\n",
      "used 72 (i.e., absence of structural plasticity created a neuron for each input sample) and, B-RSNN used 50\n",
      "output neurons respectively (i.e., 70/30 split training used 50 input samples for training where a neuron was\n",
      "created for each input). The spike generation of O-NSSN was measured as a ratio between the number of spikes\n",
      "received at the hidden layer to the number of spikes generated by the hidden layer, where the mean was recorded\n",
      "at 0.063 (σ = 0.009). This spike encoding is epitomized in Fig. 4c where the raster plot indicates the temporal\n",
      "sparseness of the spikes. When considering the trend of model accuracy over time, O-NSNN typically reached\n",
      "a prequential accuracy of 80% within 150 to 200 s of data processing commencement (the accuracy behavior\n",
      "Label PSS-14 score Number of participants\n",
      "High stress (HS) PSS > 30 6\n",
      "Medium stress (MS) 20 < PSS ≤ 29 11\n",
      "Low stress (LS) 0 < PSS ≤ 19 5\n",
      "Table 1. Participant categorization according to perceived stress (PSS-14 score).\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 5\n",
      "Vol.:(0123456789)\n",
      "www.nature.com/scientificreports/\n",
      "O-NSNN O-RSNN B-RSNN\n",
      "Participant ID Accuracy Sensitivity Accuracy Sensitivity Accuracy Sensitivity\n",
      "P1 0.94 ± 0.02 0.93 ± 0.05 0.66 ± 0.09 0.50 ± 005 0.84 ± 0.07 0.76 ± 0.12\n",
      "P2 0.93 ± 0.03 0.93 ± 0.20 0.57 ± 0.07 0.49 ± 0.09 0.72 ± 0.08 0.75 ± 0.10\n",
      "P3 0.91 ± 0.06 0.96 ± 0.70 0.60 ± 0.06 0.59 ± 0.14 0.69 ± 0.09 0.61 ± 0.15\n",
      "P4 0.90 ± 0.08 0.88 ± 0.04 0.77 ± 0.11 0.67 ± 0.22 0.93 ± 0.05 0.96 ± 0.07\n",
      "P5 0.91 ± 0.06 0.80 ± 0.06 0.66 ± 0.11 0.61 ± 0.27 0.83 ± 0.09 0.81 ± 0.14\n",
      "P6 0.92 ± 0.03 0.90 ± 0.45 0.79 ± 0.08 0.72 ± 0.18 0.88 ± 0.08 0.83 ± 0.12\n",
      "P7 0.90 ± 0.06 0.63 ± 0.04 0.59 ± 0.08 0.62 ± 0.05 0.74 ± 0.09 0.82 ± 0.11\n",
      "P8 0.94 ± 0.02 0.95 ± 0.09 0.68 ± 0.11 0.66 ± 0.25 0.88 ± 0.09 0.90 ± 0.09\n",
      "P9 0.94 ± 0.02 0.94 ± 0.07 0.76 ± 0.08 0.79 ± 0.06 0.75 ± 0.10 0.96 ±0.70\n",
      "P10 0.91 ± 0.06 0.95 ± 0.34 0.42 ± 0.07 0.48 ± 0.13 0.49 ± 0.11 0.53 ± 0.18\n",
      "P11 0.91 ± 0.07 0.96 ± 0.17 0.75 ± 0.08 0.64 ± 0.11 0.86 ± 0.08 0.84 ± 0.14\n",
      "P12 0.91 ± 0.04 0.93 ± 0.23 0.68 ± 0.08 0.76 ± 0.05 0.86 ± 0.08 0.81 ± 0.14\n",
      "P13 0.92 ± 0.08 0.90 ± 0.06 0.82 ± 0.09 0.89 ± 0.21 0.92 ± 0.06 0.95 ± 0.07\n",
      "P14 0.89 ± 0.07 0.86 ± 0.47 0.53 ± 0.10 0.54 ± 0.11 0.62 ± 0.10 0.60 ± 0.13\n",
      "P15 0.92 ± 0.04 0.93 ± 0.09 0.66 ± 0.09 0.75 ± 0.14 0.77 ± 0.10 0.79 ± 0.16\n",
      "P16 0.91 ± 0.04 0.93 ± 0.12 0.46 ± 0.10 0.36 ± 0.25 0.60 ± 0.12 0.71 ± 0.13\n",
      "P17 0.86 ± 0.13 0.95 ± 0.05 0.49 ± 0.10 0.45 ± 0.21 0.71 ± 0.10 0.79 ± 0.16\n",
      "P18 0.85 ± 0.09 0.87 ± 0.55 0.55 ± 0.09 0.55 ± 0.09 0.65 ± 0.09 0.62 ± 0.13\n",
      "P19 0.90 ± 0.07 0.91 ± 0.06 0.41 ± 0.08 0.51 ± 0.15 0.61 ± 0.08 0.60 ± 0.16\n",
      "P20 0.91 ± 0.07 0.95 ± 0.05 0.74 ± 0.10 0.64 ± 0.18 0.81 ± 0.08 0.87 ± 0.19\n",
      "P21 0.90 ± 0.12 0.93 ± 0.04 0.64 ± 0.09 0.64 ± 0.05 0.75 ± 0.09 0.62 ± 0.11\n",
      "P22 0.92 ± 0.03 0.87 ± 0.13 0.67 ± 0.09 0.53 ± 0.11 0.82 ± 0.09 0.89 ± 0.12\n",
      "Table 2. Accuracy and sensitivity comparison between online [with SP (O-NSNN) and without SP\n",
      "(O-RSNN)] and batch mode (B-RSNN) learning.\n",
      "against number of samples processed is given in Fig. 4b). An exception to this trend was noted in the case of P17\n",
      "and P21 O-NSNN models.\n",
      "O‑NSNN knowledge extraction. We also analyzed the Euclidean distance (ED) of the output neu-\n",
      "ron weight vectors and input to the hidden layer synaptic weights (i.e., STDP weights), of each individualized\n",
      "O-NSNN model. The evolved output neurons of an individualized O-NSNN model represented a certain class\n",
      "(i.e., stress, neutral or positive). The O-NSNN used this weight vector of the output neurons to predict the class\n",
      "of the incoming signals. Therefore, each ED of a sample is a numerical representation of the individual’s brain\n",
      "signal under a given stimulus. Similarly, the weights of input to the hidden layer in O-NSNN are updated in\n",
      "an unsupervised method using STDP and IP. Once all the data samples are passed through the network, the\n",
      "O-NSNN weights (i.e., input to the hidden layer) capture the spatiotemporal correlations of the input signals.\n",
      "Comparing numerical representations of brain signals. We compared the EDs between the HS, MS, and\n",
      "LS groups and found that the mean distance between neutral and critical stimuli of the HS group was 0.95\n",
      "(σ = 0.41). In contrast, the LS group’s average distance between neutral and critical stimuli was much shorter at\n",
      "0.25 (σ = 0.22). The average distance between neutral and positive stimuli of the HS group was 0.87 (σ = 0.86)\n",
      "and lower than that of the LS group’s distance of 1.86 (σ = 0.84). According to these results, the HS group’s EEG\n",
      "representations for positive stimuli did not differ to any notable extent from the EEG generated for neutral\n",
      "stimuli; this was the same for negative stimuli (i.e., under stress). However, the LS group recorded a much larger\n",
      "difference in both cases (see Fig. 5a).\n",
      "Input channel correlation. When considering the activations between input channels (i.e., using the input to\n",
      "hidden layer synaptic weights), the majority of MS participants exhibited similar activation patterns (see Fig. 5c),\n",
      "whereas the LS and HS groups exhibited irregular patterns of activation from one individual to another (see\n",
      "Fig. 5b,d). While investigating this further by examining the input synaptic weights of the hidden layer, we found\n",
      "that the HS group had higher inhibition than the LS group in the FP1 and FP2 channels (see Fig. 6). The same\n",
      "inhibitory patterns were observed for T8 but not T7. When examining the right and left-brain activations, we\n",
      "discovered that the HS group showed higher inhibition in the right hemisphere (FP2 and T8) than in the left\n",
      "hemisphere (FP1 and T7). However, in the LS group, the average difference between right and left hemisphere\n",
      "activations was significantly smaller. Moreover, higher activation was observed between FP1 and T8 than FP2\n",
      "and T7 in five out of six participants in the HS group. The opposite activation pattern was observed in four out\n",
      "of five of the participants in the LS group.\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 6\n",
      "Vol:.(1234567890)\n",
      "www.nature.com/scientificreports/\n",
      "Figure 3. Performance variation of individual models. Performance distribution obtained from 30 testing\n",
      "cycles. At each cycle the initial weights between the input to hidden layers are selected pseudo randomly\n",
      "according to gaussian distribution. S Online learning with SP, N Online learning without SP, B Batch mode\n",
      "learning without SP.\n",
      "Discussion\n",
      "This study presents Neuroplasticity Spiking Neural Network in an online learning setup for classifying the neural\n",
      "activity of healthy participants when exposed to comments that are intended to trigger different levels of mental\n",
      "states (i.e., stress, neutral, positive) and explores the link between these classifications and self-reported stress\n",
      "levels (perceived mental stress scores). This O-NSNN method produced higher pattern recognition capability\n",
      "on the fly, with increased efficiency, interpretability, and biological plausibility.\n",
      "The performance of the O‑NSNN. The O-NSNN outperformed the other SNNs (O-RSNN and B-RSNN),\n",
      "in terms of average accuracy, as shown in Table 2. When comparing the two online learning methods, O-NSNN\n",
      "(90.76%, σ = 2.09) was found to perform significantly better than O-RSNN (63.08%, σ = 11.09) (Student’s t‑test,\n",
      "α = 0.05, p = 0.005) in terms of accuracy. As per Fig. 3, the O-NSNN model produced the least performance vari-\n",
      "ation indicating higher robustness64. When considering the DEAP dataset, the O-NSNN could not outperform\n",
      "SNN and SVM techniques built for stress recognition (Table 3). The methods that outperformed the O-NSNN\n",
      "used feature engineering61 or hyperparameter o ptimization65 methods for the modelling tasks. Exploring the\n",
      "modelling mechanisms of O-NSNN, we found that the EDs of output neurons (i.e., numerical representations\n",
      "of input samples) to have better discriminative capability between the initial and final states of O-NSNN than\n",
      "in O-RSNN. This enhanced discriminative capability is presented in Fig. 7 for P1. With neurons evolving and\n",
      "self-pruning being the only difference between O-NSNN and O-RSNN; we propose this SP technique as a suc-\n",
      "cessful method for handling new classes and/or new representations of already-known classes. In other words,\n",
      "the O-NSSN approach is effective at handling concept drift.\n",
      "STDP and IP learning. In a previous study, it was reported how hidden layer neuron pruning with STDP + IP\n",
      "leads to increased robustness and efficiency of SNNs in a batch learning setup for EEG classification45. In the\n",
      "same study, hidden layer neurons with low firing probability causing classification errors were noted. In this\n",
      "study, instead of completely pruning these low-firing probability neurons, we have adopted a self-pruning\n",
      "method that stops a neuron activation for a limited period. This is achieved by increasing the neuron threshold\n",
      "voltage to the highest value found in the population. The advantage of this method is three-fold. Firstly, the inac-\n",
      "tivity of the neuron caused by threshold alteration help in reducing the number of dimensions used to represent\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 7\n",
      "Vol.:(0123456789)\n",
      "www.nature.com/scientificreports/\n",
      "Figure 4. (a) Number of output neurons evolved by O-NSNN during 30 testing cycles for each participant\n",
      "model (b) Prequential accuracy progression with the number of samples increasing (c) Sample spiking raster\n",
      "plot of the hidden layer for P1.\n",
      "an input sample at the output layer. Since classifications of the proposed O-NSNN are based on EDs calculated\n",
      "from output layer synaptic weights, part of the increase in performance may be attributed to the mitigation of the\n",
      "curse of dimentionality66. Secondly, the self-pruned neurons remain in the network to respond to salient features\n",
      "that may occur due to drifts or new data. This repurposing of neurons may account for the improvement of the\n",
      "performance of the network with time41. Thirdly, the efficiency of this pruning is superior to regular synaptic\n",
      "pruning, which requires scanning of the entire weight matrix against a t hreshold41,67.\n",
      "The efficiency of O‑NSNN. The efficiency of the O-NSNN in terms of the number of neurons used and spikes\n",
      "generated reduced drastically with the use of STDP + IP learning and self-pruning. Unlike continuous streams of\n",
      "spiking, these techniques enabled sparser spiking activity resulting inactive states most of the time (see Fig. 4c).\n",
      "When compared to STDP-only learning, STDP + IP was shown to have reduced the average spiking by 35 times\n",
      "(Student’s t‑test, α = 0.05, p = 0.008). This reduction of spikes minimizes the calculations involved from the hid-\n",
      "den to the output layer. Moreover, the O-NSNN output layer utilized 3.52-times and 2.45-times lesser neu-\n",
      "rons on average compared to O-RSNN and B-RSNN models, respectively. In comparison to the early methods\n",
      "of evolving neurons where the spiking is not regulated35,68 and the output repository grows indefinitely37, this\n",
      "method is much more suitable for memory-restricted applications.\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 8\n",
      "Vol:.(1234567890)\n",
      "www.nature.com/scientificreports/\n",
      "Figure 5. (a) Average differences between EEG samples represented by Euclidean distances. The signals during\n",
      "Neutral stimuli is selected as the baseline. (b) Spiking interaction pattern between channels for the High stress\n",
      "group (c) Spiking interaction pattern between channels for the Medium stress group (d) Spiking interaction\n",
      "pattern between channels for the Low stress group.\n",
      "Knowledge extraction. From trained O-NSNN models, HS participants showed lower activation levels\n",
      "in prefrontal channels FP1 and FP2 compared to the LS group. This was observed during the synaptic weight\n",
      "analysis of individual models, where the HS group had more inhibitory weights connected to FP1 and FP2 chan-\n",
      "nels (see Fig. 6). Moreover, the T8-connected synapses showed higher activations for the HS group (compared\n",
      "to the LS group), but this was not the case with T7-connected synapses (see Fig. 6). In terms of the channel acti-\n",
      "vation patterns, a similarity was observed among the individuals of the MS group but not in HS and LS groups\n",
      "(Fig. 5b–d). In addition, the HS group had the smallest difference between EDs (numerical representations of\n",
      "spike patterns) produced during stress and positive stimuli compared to neutral states, whereas in the LS group,\n",
      "the observation was the opposite (Fig. 5a). This suggests a lack of change in functional patterns of the brain to\n",
      "external stimuli in the HS group and, a greater change in functional patterns in the LS group. This observation\n",
      "leads to an interesting hypothesis about the relationship between acute and perceived stress. Namely, the indi-\n",
      "viduals with high perceived stress (HS group) have less discrimination between positive and negative stimuli. In\n",
      "a previous study, long-term stress has been found to alter the perception of emotional s timuli69.\n",
      "Biological plausibility. The biological plausibility of O-NSNN can be discussed in the aspects of data pro-\n",
      "cessing techniques employed and the spiking behavior observed. Firstly, the data processing techniques inspired\n",
      "by neuroscientific concepts include STDP for temporal synaptic learning38, IP for neuron spike regulation39, self-\n",
      "pruning (apoptosis) to selectively restrict activation of n eurons70, and addition of new neurons (neurogenesis)\n",
      "for retention of new k nowledge71. Secondly, the model introduced demonstrates avalanche-like spiking which is\n",
      "also found in neocortical c ircuits72. Arguably this makes O-NSNN much more biologically plausible than other\n",
      "online learning methods introduced, which do not utilize the same repertoire of plasticity techniques or show\n",
      "spiking behavior close to what is found in biology21,34,35.\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 9\n",
      "Vol.:(0123456789)\n",
      "www.nature.com/scientificreports/\n",
      "Figure 6. Cumulative weights of the synapses fanning out from respective inputs calculated according to\n",
      "perceived stress groups.\n",
      "Study Method Accuracy Sensitivity\n",
      "Bastos-Fiho et al.62 K-NN (batch mode) 0.70 –\n",
      "Shon et al.63 K-NN (batch mode) 0.72\n",
      "García-Martínez et al.61 SVM (batch mode) 0.81 –\n",
      "Weerasinghe et al.29 SNN (batch mode) 0.92 ± 0.02 –\n",
      "This study NSNN (online mode) 0.80 ± 0.05 0.79 ± 0.04\n",
      "Table 3. Performance comparison with other studies that used the same data for stressed vs relaxed brain\n",
      "signal classification.\n",
      "Figure 7. Euclidean Distance between initial(Blue) and final(Red) output neurons. The initiation process use\n",
      "the first 15 samples to evolve 15 output neurons. (a) without pruning or evolving new neurons (O-RSNN) (b)\n",
      "with pruning and evolving new neurons (O-NSNN).\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 10\n",
      "Vol:.(1234567890)\n",
      "www.nature.com/scientificreports/\n",
      "Conclusion\n",
      "This work presents a novel neural network algorithm for mental stress classification using EEG data and online\n",
      "learning. The algorithm adapts to individuals and uses functional concepts of the biological brain to learn, on\n",
      "the fly, in a resource-efficient manner. The O-NSNN algorithm introduced displayed superior performance in\n",
      "terms of accuracy, robustness, and resource efficiency over models that did not use structural plasticity.\n",
      "Our method introduced goes beyond traditional black box ANN models to reveal insights into individual\n",
      "brain dynamics for better interpretation. Improving the capability of this algorithm to recognize a higher number\n",
      "of classes under resource restrictions could potentially contribute to the applications of wearable technology for\n",
      "the detection and monitoring of mental stress.\n",
      "Methods\n",
      "Neuroplasticity spiking neural network. Here we present a description of the O-NSSN model and the\n",
      "experimental framework designed to test the model. The NSNN is a fully connected, feed-forward spiking neu-\n",
      "ral network consisting of LIF n eurons42. The input nodes can process both excitatory and inhibitory spikes.\n",
      "These nodes are connected to the hidden layer via excitatory and inhibitory synapses in which the weights are\n",
      "updated using an unsupervised STDP learning algorithm38. The hidden layer neurons operate in an adaptive\n",
      "threshold scheme in an unsupervised manner using an IP learning rule45. The hidden layer is connected to the\n",
      "output layer via excitatory synapses updated according to Spike Driven Synaptic Plasticity73 and, initiated using\n",
      "the Rank Order (RO) r ule74. The hidden layer neurons undergo a self-pruning mechanism. The third layer acts\n",
      "as the classifier and can evolve new neurons. All the hyperparameter values of the NSNN introduced are given\n",
      "in Table 4.\n",
      "Spike encoding using address event representation. AER is a biologically inspired spike encoding mechanism\n",
      "used in artificial retina a pplications44. Its simplicity, efficiency, and adaptiveness make it an attractive option\n",
      "for online applications. The temporal difference tempdiff (t)[refer Eq. (1)], between two temporarily contiguous\n",
      "data points (denoted xt and x(t−1) ) and, a user defined threshold factor f is used to calculate an adaptive spike\n",
      "threshold at each time step [refer to Eq. (2)]. If the EEG voltage value of the current time step is more than the\n",
      "threshold, an excitatory spike is emitted otherwise an inhibitory spike is emitted.\n",
      "tempdiff (t)=xt−x(t−1) (1)\n",
      "threshold=mean tempdiff (t) + f ∗std tempdiff (t) (2)\n",
      "(cid:31) (cid:30) (cid:31) (cid:31) (cid:30)(cid:30)\n",
      "Leaky integrate and fire neuron. The LIF neuron is commonly used in machine learning applications due to\n",
      "its computational tractability and the ability to produce basic spike behaviors43. Since this study involves an IP\n",
      "(adaptive voltage threshold) method, a wider variety of spiking behaviors can be produced than can be produced\n",
      "by a normal L IF43. The membrane potential change dvt of a LIF neuron can be modelled using a resistor–capaci-\n",
      "dt τ\n",
      "tor circuit and mathematically expressed using Eq. (3). In the equation, the time constant m is equal to the\n",
      "product of resistance R and capacitance C . The membrane potential is given by vt and, the input current at time\n",
      "t is given by It . The resting voltage of the neuron is given by vrest.\n",
      "Participant identifier Online learning with SP\n",
      "AER encoder f 0.7\n",
      "vthresh 0.05\n",
      "vrest 0\n",
      "LIF\n",
      "R 1\n",
      "C 10\n",
      "A+ 0.001\n",
      "A− 0.001\n",
      "τ pos 10\n",
      "STDP\n",
      "τ neg 10\n",
      "wmax 0.5\n",
      "wmin − 0.5\n",
      "θ pos 0.001\n",
      "IP\n",
      "θ neg 0.000001\n",
      "Pruner sp thresh 1\n",
      "α 1\n",
      "Classifier mod 0.8\n",
      "drift 0.001\n",
      "Table 4. O-NSNN hyperparameters.\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 11\n",
      "Vol.:(0123456789)\n",
      "www.nature.com/scientificreports/\n",
      "τ\n",
      "mdvt\n",
      "=vrest−vt+RIt,τ m =RC (3)\n",
      "dt\n",
      "Unsupervised learning. In the O-NSSN, the unsupervised weight update strategy S TDP38 is accompanied by an\n",
      "IP rule45 that adapts the threshold of hidden layer neurons individually. This combination of plasticity is a key\n",
      "factor in maintaining firing homeostasis and enhancing SNN performance in terms of classification accuracy\n",
      "and efficiency45,47,75.\n",
      "F(�t)=A+exp(−�t/τ\n",
      "pos)�t>0 (4)\n",
      "F(�t)=−A_exp(�t/τ\n",
      "neg)�t<0 (5)\n",
      "b q\n",
      "�wij = F(t jm−t in) (6)\n",
      "a p\n",
      "(cid:31)(cid:31)\n",
      "Equations (4) and (5) represent STDP according to Long-Term Potentiation (LTP) and Long-Term Depre-\n",
      "ciation (LTD) respectively38. Both equations are functions of the time difference (cid:31)t between spikes. In Eq. (6)\n",
      "the pre-synaptic neuron is denoted by i and the post-synaptic by j . If j fires before i , (cid:31)t is positive leading to\n",
      "LTP. A reversed firing sequence leads to LTD. In Eqs. (4) and (5), the positive and negative time constants are\n",
      "τ τ\n",
      "given by pos and neg respectively. These time constants are predetermined windows of time used for synaptic\n",
      "modifications. A+ and A_ terms determine the maximum synaptic modification. The cumulative weight change\n",
      "(cid:31)Wij is calculated using the spike timing of each pre-synaptic neuron from p to q and each post-synaptic neuron\n",
      "spiking from a to b . The instantaneous spike time of each post-synaptic neuron is given by tm and each pre-\n",
      "j\n",
      "synaptic neuron by tn.\n",
      "i\n",
      "The IP rule operates simultaneously with STDP according to the two equations defined in (7). Here, the\n",
      "first expression of Eq. (7) is used to upregulate the neuron voltage thresholds and, the second to down-regulate.\n",
      "v thr(t)= v t vh tr h( rt (− t−1) 1)+ −N Nθ p θo ns ev gi vn ii nt, its ,( ot t− he1 rw) i= se1 (7)\n",
      "(cid:31)\n",
      "The threshold voltage of a neuron at time t is given by v thr(t) . If the neuron fired in the previous time step and\n",
      "satisfies the condition s(t−1)=1 , then a fraction of the initial voltage vinit is added to the threshold voltage of\n",
      "the previous time step v thr(t−1) . This fraction is calculated using the product of the positive learning rate θ pos\n",
      "and the number of neurons in the hidden layer N . If a spike did not occur in the previous time step, then the\n",
      "θ\n",
      "threshold voltage is lowered using the negative learning rate neg . The two learning rates are determined based\n",
      "on the highest neuron activation and lowest information entropy45 after each sample propagation.\n",
      "Structural plasticity. The addition of new neurons in the output layer and self-pruning of the hidden layer are\n",
      "the two key SP techniques incorporated in the NSNN algorithm. There are no neurons in the output layer at first.\n",
      "During the initiation process, a predefined number of neurons evolved. The number of samples used to evolve\n",
      "these initial neurons was 15 for the NSNN in this study. This set of neurons remains in the network and gets their\n",
      "weights updated at each sample pass. Since the NSNN operates under the test-then-train regime, if an error is\n",
      "made during the test phase, a new neuron is evolved in the following training phase. Here, an error symbolizes\n",
      "the emergence of a new class or a representational change in an already known class caused by concept drift76.\n",
      "Moreover, self-pruning also takes place in the hidden layer if an error is identified in the previous time step. This\n",
      "self-pruning is executed on neurons with low spiking probability since they can cause poor g eneralization45.\n",
      "W jk(init) =α.modorder(j,k) (8)\n",
      "n\n",
      "W jk(t)=W jk(init)+ d (9)\n",
      "t=1\n",
      "(cid:31)\n",
      "The synaptic weights from the hidden to the output layer are initiated according to the RO rule given in\n",
      "Eq. (8). The initial weight between j pre-synaptic neuron and k post-synaptic neuron W jk(init) , is determined\n",
      "using a learning parameter α and an exponent of mod . The modulation factor mod is determined based on the\n",
      "importance of the spike order. For the first spike to arrive at the synapse, order(j,k) starts at 0, thereby allocating\n",
      "the highest possible weight and increases as the spikes arrive at other neurons (i.e., decreases W jk(init) ). Thereaf-\n",
      "ter, a drift parameter d is used to increase or decrease the initial weight to form a weight value at time t , W jk(t).\n",
      "Performance evaluation. To evaluate the performance in online learning, we used the prequential accu-\n",
      "racy metric76 with the test-then-train approach22. In test-then-train, a sample is tested first before training. This\n",
      "method minimizes the memory cost since samples need not be held in memory. By applying prequential mem-\n",
      "ory with this approach, accuracy can be updated incrementally. The accuracies for online learning stated in the\n",
      "study are the final accuracy performance after 360 s or 72 samples.\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 12\n",
      "Vol:.(1234567890)\n",
      "www.nature.com/scientificreports/\n",
      "ACCpre(t)= (cid:31)Accpre(t−A 1c )c +pre( At c) c, pi rf et ( tt −)= − tinAt itci +n ci p 1t re(t−1) ,else (10)\n",
      "In Eq. (10), the classification accuracy of the NSNN at time t is given by ACCpre(t) . Here, tinit represents the\n",
      "initial time point which is taken as the reference time point. For the batch learning experiments (i.e., B-RSNN),\n",
      "we used the standard accuracy metric which is defined as the ratio of the number of correct predictions over\n",
      "the total number of predictions77.\n",
      "Ethics approval and consent to participate. All experiments were performed in accordance with the\n",
      "relevant guidelines and regulations. The Auckland University of Technology Ethics Committee (AUTEC) pro-\n",
      "vided approval for the study on 2nd October 2019 (Approval identity number: 19/231). All participants were\n",
      "provided with a detailed informed consent form, which was also explained verbally, detailing the objectives,\n",
      "activities and consequences related to the study. All participants provided the signed informed consent form\n",
      "prior to data collection.\n",
      "EEG Data. The participant group consisted of 12 males with an average age of 27.92 (σ = 3.09) and 10 females\n",
      "with an average age of 25.9 (σ = 8.20). The EEG data were recorded over three sessions in a sound-attenuated\n",
      "room with a gap of at least one day between each session to prevent carry-over effects. At each session, the\n",
      "participant followed a sequence of steps: starting with completing the PSS-14 survey, recording two minutes of\n",
      "resting EEG, recording EEG while listening to an audio of either critical, neutral or positive comments, followed\n",
      "by a recording of two minutes of resting EEG. The type of audio comments for the session was selected randomly.\n",
      "Each comment lasted from 10 to 15 s and 40 such comments were made to listen through earphones during each\n",
      "session. It was presumed that critical comments would induce stress based on the result of previous studies51–53.\n",
      "However, it is noted that all participants may not be stressed to the same level by critical audio comments in an\n",
      "experimental setup. Therefore, the sensitivity to each comment was assessed using measurements of arousal and\n",
      "relevance on an 11-point Likert scale.\n",
      "The 120 auditory comments used for the study were recordings of male and female native English speakers\n",
      "specifically trained to emphasize critical, neutral and positive comments through pitch and t one55,56. The critical\n",
      "and positive comments were typical remarks that one would hear from a close family member, and the neutral\n",
      "comments were factual statements that had no relevance to the participant. Samples of such comments include,\n",
      "“you are lazy and never finish anything you start! you’ve had chances but didn’t go through with it” (Critical\n",
      "comment); “you are good at organising things and paying attention to detail.” (Positive comment); “the Emu\n",
      "is the largest native bird in Australia, with long neck and legs” (Neutral comment). Details of these comments\n",
      "have been published previously55,56.\n",
      "EEG recording was performed with a SynAmps amplifier and a 62-channel QuickCap with electrodes config-\n",
      "ured in the international 10–20 system. Electrodes channels were: FP1, FPZ, FP2, AF3, AF4, F7, F5, F3, F1, FZ,\n",
      "F2, F4, F6, F8, FT7, FC5, FC3, FC1, FCZ, FC2, FC4, FC6, FT8, T7, C5, C3, C1, CZ, C2, C4, C6, T8, TP7, CP5,\n",
      "CP3, CP1, CPZ, CP2, CP4, CP6, TP8, P7, P5, P3, P1, PZ, P2, P4, P6, P8, PO7, PO5, PO3, POZ, PO4, PO6, PO8,\n",
      "CB1, O1, OZ, O2, CB2. Data was recorded at 1000 Hz. Using multiple electrodes is a better approach than using\n",
      "a single electrode when assessing multiple levels of stress58. However, processing all the channels will require\n",
      "greater processing power. Therefore, FP1, FP2, T7 and T8 specific electrodes were selected. The selection of\n",
      "frontal electrodes were based on a previous EEG feature selection study conducted on stress classification which\n",
      "reported higher accuracy levels with FP1 and FP258. Moreover, since the stress stimulations were auditory, T7\n",
      "and T8 were used in an attempt to capture the dynamics of the auditory cortex. Previously, emotional auditory\n",
      "stimuli had been found to evoke different levels of valence in individuals that co-varied significantly with EEG\n",
      "signals generated by the auditory region78 and, negative valence is found to be strongly connected with stress8.\n",
      "EEG data preprocessing was performed in MATLAB 2019a (The Mathworks, Inc)79 using custom scripts\n",
      "that involved functions from EEGLAB plugin80. Data were down-sampled offline to 256 Hz. A high-pass finite\n",
      "impulse response (FIR) filter at 0.01 Hz and a low-pass FIR filter at 50 Hz were applied. A baseline correction\n",
      "was not applied separately since the high-pass filter with low cutoff frequencies are found to rectify the baseline\n",
      "d rift81. Using the CleanLine function80, line noise was removed before data were manually inspected for the\n",
      "removal of bad channels (flat or extremely noisy). The removed channels were interpolated before an independent\n",
      "component analysis was performed, to decompose the sample, using the runica f unction80 from the MATLAB\n",
      "ICA Toolbox for Psychophysiological Data Analysis82. The independent components derived from ICA were\n",
      "inspected and muscular and ocular artifacts were removed from the data based on their activity spectra and\n",
      "scalp topographies. After the preprocessing steps, the last five seconds of the voltage signal was selected (Each\n",
      "original EEG signal consisted of 10 to 15 s. i.e., the stimulus presentation time). This extracted portion of the\n",
      "voltage signal was then converted into temporal spikes using AER protocol44 before feeding the SNNs. No other\n",
      "feature engineering or extractions were carried out.\n",
      "Data availability\n",
      "The main dataset used in the current study is available from the corresponding author on reasonable request.\n",
      "Received: 9 July 2022; Accepted: 3 May 2023\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 13\n",
      "Vol.:(0123456789)\n",
      "www.nature.com/scientificreports/\n",
      "References\n",
      "1. Selye, H. The stress syndrome. Am. J. Nurs. 65, 97 (1965).\n",
      "2. Epel, E. S. et al. More than a feeling: A unified view of stress measurement for population science. Front. Neuroendocrinol. 49,\n",
      "146–169 (2018).\n",
      "3. Crowley, O. V. et al. The interactive effect of change in perceived stress and trait anxiety on vagal recovery from cognitive challenge.\n",
      "Int. J. Psychophysiol. 82, 225–232 (2011).\n",
      "4. O’Connor, D. B., Thayer, J. F. & Vedhara, K. Stress and health: A review of psychobiological processes. Annu. Rev. Psychol. 72,\n",
      "663–688 (2021).\n",
      "5. Wu, J., Feng, M., Liu, Y., Fang, H. & Duan, H. The relationship between chronic perceived stress and error processing: Evidence\n",
      "from event-related potentials. Sci. Rep. 9, 11605 (2019).\n",
      "6. Arnsten, A. F. T. Stress signalling pathways that impair prefrontal cortex structure and function. Nat. Rev. Neurosci. 10, 410–422\n",
      "(2009).\n",
      "7. Lawrence, D. Central/peripheral nervous system and immune responses. Toxicology 142, 189–201 (2000).\n",
      "8. Seo, S.-H. & Lee, J.-T. Stress and EEG. In Convergence and Hybrid Information Technologies (InTech, 2010). https://d oi.o rg/1 0.\n",
      "5772/9 651\n",
      "9. Jin, P. Efficacy of Tai Chi, brisk walking, meditation, and reading in reducing mental and emotional stress. J. Psychosom. Res. 36,\n",
      "361–370 (1992).\n",
      "10. Lerner, J. S., Dahl, R. E., Hariri, A. R. & Taylor, S. E. Facial expressions of emotion reveal neuroendocrine and cardiovascular stress\n",
      "responses. Biol. Psychiatry 61, 253–260 (2007).\n",
      "11. Lundberg, U. et al. Psychophysiological stress and emg activity of the trapezius muscle. Int. J. Behav. Med. 1, 354–370 (1994).\n",
      "12. Giannakakis, G. et al. Review on psychological stress detection using biosignals. IEEE Trans. Affect. Comput. 13, 440–460 (2022).\n",
      "13. Saeed, S. M. U., Anwar, S. M., Khalid, H., Majid, M. & Bagci, U. EEG based classification of long-term stress using psychological\n",
      "labeling. Sensors 20, 1–15 (2020).\n",
      "14. Zhang, Y., Wang, Q., Chin, Z. Y. & Keng Ang, K. Investigating different stress-relief methods using Electroencephalogram (EEG).\n",
      "In Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS, vols 2020\n",
      "(2020)\n",
      "15. Subhani, A. R., Mumtaz, W., Saad, M. N. B. M., Kamel, N. & Malik, A. S. Machine learning framework for the detection of mental\n",
      "stress at multiple levels. IEEE Access 5, 13545–13556 (2017).\n",
      "16. Betti, S. et al. Evaluation of an integrated system of wearable physiological sensors for stress monitoring in working environments\n",
      "by using biological markers. IEEE Trans. Biomed. Eng. 65, 1748–1758 (2018).\n",
      "17. Khosrowabadi, R., Quek, C., Ang, K. K., Tung, S. W. & Heijnen, M. A Brain-computer interface for classifying EEG correlates of\n",
      "chronic mental stress. Proc. Int. Jt. Conf. Neural Netw. https://d oi.o rg/1 0.1 109/I JCNN.2 011.6 03329 7 (2011).\n",
      "18. Saidatul, A., Paulraj, M. P., Yaacob, S. & Yusnita, M. A. Analysis of EEG signals during relaxation and mental stress condition\n",
      "using AR modeling techniques. In Proceedings 2011 IEEE International Conference on Control System, Computing and Engineering,\n",
      "ICCSCE 2011, 477–481 (2011). https://d oi.o rg/1 0.1 109/I CCSCE.2 011.6 19057 3\n",
      "19. Domingos, P. & Hulten, G. A general framework for mining massive data streams. J. Comput. Graph. Stat. 12, 945–949 (2003).\n",
      "20. Katmah, R. et al. A review on mental stress assessment methods using EEG signals. Sensors 21, 5043 (2021).\n",
      "21. Lobo, J. L., Laña, I., Del Ser, J., Bilbao, M. N. & Kasabov, N. Evolving spiking neural networks for online learning over drifting data\n",
      "streams. Neural Netw. 108, 1–19 (2018).\n",
      "22. Lobo, J. L., Del Ser, J., Bifet, A. & Kasabov, N. Spiking neural networks and online learning: An overview and perspectives. Neural\n",
      "Netw. 121, 88–100 (2020).\n",
      "23. Maass, W. Networks of spiking neurons: The third generation of neural network models. Neural Netw. 10, 1659–1671 (1997).\n",
      "24. Zuo, F. et al. Habituation based synaptic plasticity and organismic learning in a quantum perovskite. Nat. Commun. 8, 240 (2017).\n",
      "25. Panda, P. & Roy, K. Unsupervised regenerative learning of hierarchical features in spiking deep networks for object recognition.\n",
      "Proc. Int. Jt. Conf. Neural Netw. 2016, 299–306 (2016).\n",
      "26. Kheradpisheh, S. R., Ganjtabesh, M., Thorpe, S. J. & Masquelier, T. STDP-based spiking deep convolutional neural networks for\n",
      "object recognition. Neural Netw. 99, 56–67 (2018).\n",
      "27. Masquelier, T. & Thorpe, S. J. Unsupervised learning of visual features through spike timing dependent plasticity. PLoS Comput.\n",
      "Biol. 3, e31 (2007).\n",
      "28. Bohte, S. M., Kok, J. N. & La Poutré, H. Error-backpropagation in temporally encoded networks of spiking neurons. Neurocomput‑\n",
      "ing 48, 17–37 (2002).\n",
      "29. Weerasinghe, M. M. A., Espinosa-Ramos, J. I., Wang, G. Y. & Parry, D. Incorporating structural plasticity approaches in spiking\n",
      "neural networks for EEG modelling. IEEE Access 9, 117338–117348 (2021).\n",
      "30. Legenstein, R., Naeger, C. & Maass, W. What can a neuron learn with spike-timing-dependent plasticity?. Neural Comput. 17,\n",
      "2337–2382 (2005).\n",
      "31. Gütig, R. & Sompolinsky, H. The tempotron: A neuron that learns spike timing-based decisions. Nat. Neurosci. https://d oi.o rg/1 0.\n",
      "1038/n n1643 (2006).\n",
      "32. Florian, R. V. The Chronotron: A neuron that learns to fire temporally precise spike patterns. PLoS ONE 7, e40233 (2012).\n",
      "33. Wysoski, S. G., Benuskova, L. & Kasabov, N. Fast and adaptive network of spiking neurons for multi-view visual pattern recogni-\n",
      "tion. Neurocomputing 71, 2563–2575 (2008).\n",
      "34. Wang, J., Belatreche, A., Maguire, L. & McGinnity, T. M. An online supervised learning method for spiking neural networks with\n",
      "adaptive structure. Neurocomputing 144, 526–536 (2014).\n",
      "35. Dora, S., Subramanian, K., Suresh, S. & Sundararajan, N. Development of a self-regulating evolving spiking neural network for\n",
      "classification problem. Neurocomputing 171, 1216–1229 (2016).\n",
      "36. Pardey, J., Roberts, S. & Tarassenko, L. A review of parametric modelling techniques for EEG analysis. Med. Eng. Phys. 18, 2–11\n",
      "(1996).\n",
      "37. Schliebs, S. & Kasabov, N. Evolving spiking neural network: A survey. Evol. Syst. 4, 87–98 (2013).\n",
      "38. Bi, G. & Poo, M. Synaptic Modifications in cultured hippocampal neurons: Dependence on spike timing, synaptic strength, and\n",
      "postsynaptic cell type. J. Neurosci. 18, 10464–10472 (1998).\n",
      "39. Desai, N. S., Rutherford, L. C. & Turrigiano, G. G. Plasticity in the intrinsic excitability of cortical pyramidal neurons. Nat. Neurosci.\n",
      "2, 515–520 (1999).\n",
      "40. Aimone, J. B. Computational modeling of adult neurogenesis. Cold Spring Harb. Perspect. Biol. 8, a018960 (2016).\n",
      "41. Shi, Y., Nguyen, L., Oh, S., Liu, X. & Kuzum, D. A soft-pruning method applied during training of spiking neural networks for\n",
      "in-memory computing applications. Front. Neurosci. 13, 1–13 (2019).\n",
      "42. Gerstner, W. & Kistler, W. M. Spiking Neuron Models (Cambridge University Press, 2002). https://d oi.o rg/1 0.1 017/C BO978 0511\n",
      "815706.\n",
      "43. Izhikevich, E. M. Which model to use for cortical spiking neurons?. IEEE Trans. Neural Netw. 15, 1063–1070 (2004).\n",
      "44. Delbruck, T. & Lichtsteiner, P. Fast sensory motor control based on event-based hybrid neuromorphic-procedural system. In 2007\n",
      "IEEE International Symposium on Circuits and Systems 845–848 (IEEE, 2007). https://d oi.o rg/1 0.1 109/I SCAS.2 007.3 78038.\n",
      "45. Weerasinghe, M. M. A., Parry, D., Wang, G. & Whalley, J. Ensemble Plasticity and Network Adaptability in SNNs (Springer, 2022).\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 14\n",
      "Vol:.(1234567890)\n",
      "www.nature.com/scientificreports/\n",
      "46. Chen, J. Y. et al. Heterosynaptic plasticity prevents runaway synaptic dynamics. J. Neurosci. 33, 15915–15929 (2013).\n",
      "47. Diehl, P. U. & Cook, M. Unsupervised learning of digit recognition using spike-timing-dependent plasticity. Front. Comput.\n",
      "Neurosci. 9, 99 (2015).\n",
      "48. Savin, C., Joshi, P. & Triesch, J. Independent component analysis in spiking neurons. PLoS Comput. Biol. 6, e1000757 (2010).\n",
      "49. Kasabov, N., Dhoble, K., Nuntalid, N. & Indiveri, G. Dynamic evolving spiking neural networks for on-line spatio- and spectro-\n",
      "temporal pattern recognition. Neural Netw. 41, 188–201 (2013).\n",
      "50. Westman, J. C. & Walters, J. R. Noise and stress: A comprehensive approach. Environ. Health Perspect. 41, 291–309 (1981).\n",
      "51. Wegge, J., Vogt, J. & Wecking, C. Customer-induced stress in call centre work: A comparison of audio- and videoconference. J.\n",
      "Occup. Organ. Psychol. 80, 693–712 (2007).\n",
      "52. Tops, S., Habel, U., Abel, T., Derntl, B. & Radke, S. The verbal interaction social threat task: A new paradigm investigating the\n",
      "effects of social rejection in men and women. Front. Neurosci. 13, 830 (2019).\n",
      "53. Lobbestael, J., Arntz, A. & Wiers, R. W. How to push someone’s buttons: a comparison of four anger-induction methods. Cogn.\n",
      "Emot. 22, 353–373 (2008).\n",
      "54. Er, M. B., Çiğ, H. & Aydilek, İB. A new approach to recognition of human emotions using brain signals and music stimuli. Appl.\n",
      "Acoust. 175, 107840 (2021).\n",
      "55. Premkumar, P., Dunn, A. K., Onwumere, J. & Kuipers, E. Sensitivity to criticism and praise predicts schizotypy in the non-clinical\n",
      "population: The role of affect and perceived expressed emotion. Eur. Psychiatry 55, 109–115 (2019).\n",
      "56. Wang, G. Y., Premkumar, P., Lee, C. Q. & Griffiths, M. D. The role of criticism in expressed emotion among psychoactive substance\n",
      "users: An experimental vignette study. Int. J. Ment. Health Addict. 21, 258–272. https://d oi.o rg/1 0.1 007/s 11469-0 21-0 0591-2 (2021).\n",
      "57. Cohen, S., Kamarck, T. & Mermelstein, R. A global measure of perceived stress. J. Health Soc. Behav. 24, 385 (1983).\n",
      "58. Attallah, O. An effective mental stress state detection and evaluation system using minimum number of frontal brain electrodes.\n",
      "Diagnostics 10, 292 (2020).\n",
      "59. Dawid, A. P. & Vovk, V. G. Prequential probability: Principles and properties. Bernoulli 5, 125 (1999).\n",
      "60. Koelstra, S. et al. DEAP: A database for emotion analysis; using physiological signals. IEEE Trans. Affect. Comput. 3, 18–31 (2012).\n",
      "61. García-Martínez, B., Martínez-Rodrigo, A., Zangróniz, R., Pastor, J. M. & Alcaraz, R. Symbolic analysis of brain dynamics detects\n",
      "negative stress. Entropy 19, 196 (2017).\n",
      "62. Bastos-Filho, T. F., Ferreira, A., Atencio, A. C., Arjunan, S. & Kumar, D. Evaluation of feature extraction techniques in emotional\n",
      "state recognition. In 2012 4th International Conference on Intelligent Human Computer Interaction (IHCI), 1–6 (IEEE, 2012). https://\n",
      "doi.o rg/1 0.1 109/I HCI.2 012.6 48186 0.\n",
      "63. Shon, D. et al. Emotional stress state detection using genetic algorithm-based feature selection on EEG signals. Int. J. Environ. Res.\n",
      "Public Health 15, 2461 (2018).\n",
      "64. Navlakha, S., Bar-Joseph, Z. & Barth, A. L. Network design and the brain. Trends Cogn. Sci. 22, 64–78 (2018).\n",
      "65. Weerasinghe, M. M. A., Wang, G. & Parry, D. Emotional stress classification using spiking neural networks. Psychol. Neurosci.\n",
      "https://d oi.o rg/1 0.1 037/p ne000 0294 (2022).\n",
      "66. Marimont, R. B. & Shapiro, M. B. Nearest neighbour searches and the curse of dimensionality. IMA J. Appl. Math. 24, 59–70 (1979).\n",
      "67. Rathi, N., Panda, P. & Roy, K. STDP-based pruning of connections and weight quantization in spiking neural networks for energy-\n",
      "efficient recognition. IEEE Trans. Comput. Des. Integr. Circuits Syst. 38, 668–677 (2019).\n",
      "68. Wang, J., Belatreche, A., Maguire, L. P. & McGinnity, T. M. SpikeTemp: an enhanced rank-order-based learning approach for\n",
      "spiking neural networks with adaptive structure. IEEE Trans. Neural Netw. Learn. Syst. 28, 30–43 (2017).\n",
      "69. Khosrowabadi, R. Stress and perception of emotional stimuli: Long-term stress rewiring the brain. Basic Clin. Neurosci. 9, 107\n",
      "(2018).\n",
      "70. Roth, K. A. & D’Sa, C. Apoptosis and brain development. Ment. Retard. Dev. Disabil. Res. Rev. 7, 261–266 (2001).\n",
      "71. Eriksson, P. S. et al. Neurogenesis in the adult human hippocampus. Nat. Med. 4, 1313–1317 (1998).\n",
      "72. Beggs, J. M. & Plenz, D. Neuronal Avalanches in neocortical circuits. J. Neurosci. 23, 11167–11177 (2003).\n",
      "73. Fusi, S., Annunziato, M., Badoni, D., Salamon, A. & Amit, D. J. Spike-driven synaptic plasticity: Theory, simulation, VLSI imple-\n",
      "mentation. Neural Comput. 12, 2227–2258 (2000).\n",
      "74. Thorpe, S. & Gautrais, J. Rank order coding. In Computational Neuroscience 113–118 (Springer, 1998). https://d oi.o rg/1 0.1 007/\n",
      "978-1-4 615-4 831-7_1 9.\n",
      "75. Hao, Y., Huang, X., Dong, M. & Xu, B. A biologically plausible supervised learning method for spiking neural networks using the\n",
      "symmetric STDP rule. Neural Netw. 121, 387–395 (2020).\n",
      "76. Minku, L. L. & Yao, X. DDD: A new ensemble approach for dealing with concept drift. IEEE Trans. Knowl. Data Eng. 24, 619–633\n",
      "(2012).\n",
      "77. Urbanowicz, R. J. & Moore, J. H. ExSTraCS 2.0: Description and evaluation of a scalable learning classifier system. Evol. Intell. 8,\n",
      "89–116 (2015).\n",
      "78. Daly, I. et al. Electroencephalography reflects the activity of sub-cortical brain regions during approach-withdrawal behaviour\n",
      "while listening to music. Sci. Rep. 9, 9415 (2019).\n",
      "79. MATLAB. 9.7.0.1190202 (R2019a). (The MathWorks Inc., 2019).\n",
      "80. Delorme, A. & Makeig, S. EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent\n",
      "component analysis. J. Neurosci. Methods 134, 9–21 (2004).\n",
      "81. Groppe, D. M., Makeig, S. & Kutas, M. Identifying reliable independent components via split-half comparisons. Neuroimage 45,\n",
      "1199–1211 (2009).\n",
      "82. Makeig, S. & Al., E. ICA Toolbox for Psychophysiological Research (Version 3.4). https://s ccn.u csd.e du/~ scott/i ca-d ownlo ad-f orm.\n",
      "html (2000).\n",
      "Acknowledgements\n",
      "We acknowledge the support of the Sri Lanka Technological Campus (SLTC) and the Department of Computer\n",
      "Science and Software Engineering of Auckland University of Technology (AUT) in conducting and funding\n",
      "this study.\n",
      "Author contributions\n",
      "M.W. developed the learning algorithms, designed the experiments, analysed the data, and wrote the manuscript.\n",
      "G.W. designed the data collection and M.C. acquired the data. J.W. and G.W. supervised the research. All authors\n",
      "reviewed the manuscript and provided feedback.\n",
      "Competing interests\n",
      "The authors declare no competing interests.\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 15\n",
      "Vol.:(0123456789)\n",
      "www.nature.com/scientificreports/\n",
      "Additional information\n",
      "Correspondence and requests for materials should be addressed to M.M.A.W.\n",
      "Reprints and permissions information is available at www.nature.com/reprints.\n",
      "Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\n",
      "institutional affiliations.\n",
      "Open Access This article is licensed under a Creative Commons Attribution 4.0 International\n",
      "License, which permits use, sharing, adaptation, distribution and reproduction in any medium or\n",
      "format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the\n",
      "Creative Commons licence, and indicate if changes were made. The images or other third party material in this\n",
      "article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the\n",
      "material. If material is not included in the article’s Creative Commons licence and your intended use is not\n",
      "permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from\n",
      "the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n",
      "© The Author(s) 2023\n",
      "Scientific Reports | (2023) 13:14962 | https://doi.org/10.1038/s41598-023-34517-w 16\n",
      "Vol:.(1234567890)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_path = Path(\"./data/pdfs\")\n",
    "client = ModelClient()\n",
    "\n",
    "responses = []\n",
    "for pdf in list(pdf_path.glob(\"*.pdf\"))[6:7]:\n",
    "    print(pdf)\n",
    "    paper_text = extract_text_from_pdf(pdf)\n",
    "    print(paper_text)\n",
    "    prompt = get_prompt(paper_text)\n",
    "    response = client.ask_llama(prompt)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper Title: A biologically plausible supervised learning method for spiking neural networks using the symmetric STDP rule,\n",
      "Decision: Exclude,\n",
      "Reason: Not original research,\n",
      "Note: \"This study is a follow-up of our previous work on spiking neural networks (SNNs) and uses similar approaches to improve the performance of SNNs.\"\n"
     ]
    }
   ],
   "source": [
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
